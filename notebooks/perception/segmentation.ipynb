{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bde9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import to_tensor, resize\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Paths\n",
    "basedir1 = \"/home/shared/data_raw/LAC/semantics_map1_preset1\"\n",
    "basedir2 = \"/home/shared/data_raw/LAC/semantics_map2_preset12_auto\"\n",
    "\n",
    "# Semantic mapping (RGB)\n",
    "semantic_colors = {\n",
    "    (250, 170, 30): 0,  # Fiducials\n",
    "    (108, 59, 42): 1,  # Rocks\n",
    "    (110, 190, 160): 2,  # Lander\n",
    "    (81, 0, 81): 3,  # Ground\n",
    "    (0, 0, 0): 4,  # Sky\n",
    "}\n",
    "\n",
    "color_map = np.zeros((256, 256, 3), dtype=np.uint8)\n",
    "color_to_index = {tuple(k): v for k, v in semantic_colors.items()}\n",
    "\n",
    "\n",
    "def rgb_to_class(mask_rgb):\n",
    "    mask_np = np.array(mask_rgb)\n",
    "    class_mask = np.zeros(mask_np.shape[:2], dtype=np.uint8)\n",
    "    for rgb, idx in color_to_index.items():\n",
    "        class_mask[np.all(mask_np == rgb, axis=-1)] = idx\n",
    "    return class_mask\n",
    "\n",
    "\n",
    "class SegDataset(Dataset):\n",
    "    def __init__(self, img_paths, mask_paths, downscale_factor=2):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.downscale_factor = downscale_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.img_paths[idx])\n",
    "        mask = cv2.imread(self.mask_paths[idx])\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        H, W, _ = img.shape\n",
    "        H_ds = (H // self.downscale_factor) // 32 * 32\n",
    "        W_ds = (W // self.downscale_factor) // 32 * 32\n",
    "\n",
    "        img_resized = resize(Image.fromarray(img), (H_ds, W_ds))\n",
    "        mask_resized = resize(\n",
    "            Image.fromarray(rgb_to_class(mask)), (H_ds, W_ds), interpolation=Image.NEAREST\n",
    "        )\n",
    "\n",
    "        img_tensor = to_tensor(img_resized)\n",
    "        mask_tensor = torch.from_numpy(np.array(mask_resized)).long()\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "\n",
    "cameras = [\"FrontLeft\", \"FrontRight\", \"Left\", \"Right\"]\n",
    "\n",
    "\n",
    "def get_paths(basedirs):\n",
    "    mask_paths_train, mask_paths_test = [], []\n",
    "    img_paths_train, img_paths_test = [], []\n",
    "\n",
    "    for basedir in basedirs:\n",
    "        all_filenames = sorted(\n",
    "            [\n",
    "                f\"{c}/{f}\"\n",
    "                for c in cameras\n",
    "                for f in os.listdir(os.path.join(basedir, c))\n",
    "                if f.endswith(\".png\")\n",
    "            ],\n",
    "            key=lambda x: int(x.split(\"/\")[-1].split(\".\")[0]),\n",
    "        )\n",
    "        np.random.seed(42)\n",
    "        # np.random.shuffle(all_filenames)\n",
    "        split_idx = int(0.8 * len(all_filenames))\n",
    "        train_files = all_filenames[:split_idx]\n",
    "        test_files = all_filenames[split_idx:]\n",
    "        img_paths_train.extend([os.path.join(basedir, f) for f in train_files])\n",
    "        img_paths_test.extend([os.path.join(basedir, f) for f in test_files])\n",
    "        mask_paths_train.extend(\n",
    "            [\n",
    "                os.path.join(basedir, f.replace(f.split(\"/\")[0], f.split(\"/\")[0] + \"_semantic\"))\n",
    "                for f in train_files\n",
    "            ]\n",
    "        )\n",
    "        mask_paths_test.extend(\n",
    "            [\n",
    "                os.path.join(basedir, f.replace(f.split(\"/\")[0], f.split(\"/\")[0] + \"_semantic\"))\n",
    "                for f in test_files\n",
    "            ]\n",
    "        )\n",
    "    return img_paths_train, mask_paths_train, img_paths_test, mask_paths_test\n",
    "\n",
    "\n",
    "img_paths_train, mask_paths_train, img_paths_test, mask_paths_test = get_paths([basedir1, basedir2])\n",
    "\n",
    "train_ds = SegDataset(img_paths_train, mask_paths_train)\n",
    "test_ds = SegDataset(img_paths_test, mask_paths_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=24, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=4, num_workers=24, pin_memory=True)\n",
    "\n",
    "model_names = [\n",
    "    \"Unet\",\n",
    "    \"UnetPlusPlus\",\n",
    "    \"FPN\",\n",
    "    \"PSPNet\",\n",
    "    \"DeepLabV3\",\n",
    "    \"DeepLabV3Plus\",\n",
    "    \"Linknet\",\n",
    "    \"MAnet\",\n",
    "    \"PAN\",\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "results = {}\n",
    "\n",
    "name = \"UnetPlusPlus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = cv2.imread(img_paths_test[-1]), cv2.imread(mask_paths_test[-1])\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(mask)\n",
    "plt.show()\n",
    "# Mask based on intensity\n",
    "mask_intensity = np.mean(img, axis=-1) > 50\n",
    "plt.imshow(mask_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n==> Training {name}\")\n",
    "model = (\n",
    "    getattr(smp, name)(\n",
    "        encoder_name=\"resnet34\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "        classes=5,\n",
    "    )\n",
    "    .to(device)\n",
    "    .to(memory_format=torch.channels_last)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17377a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for img, mask in test_loader:\n",
    "        img = img.to(device)\n",
    "        out = model(img)\n",
    "        pred = out.argmax(1).squeeze().cpu()\n",
    "\n",
    "        for i in range(pred.shape[0]):\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(img[i].permute(1, 2, 0).cpu())\n",
    "            plt.title(\"Input\")\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(mask[i].squeeze())\n",
    "            plt.title(\"Ground Truth\")\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(pred[i])\n",
    "            plt.title(\"Prediction\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = torch.tensor([1.0, 5.0, 1.0, 1.0, 1.0], device=device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    total_loss = 0\n",
    "    for images, masks in tqdm(train_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.amp.autocast(\"cuda\"):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "eval_loss = 0\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(test_loader):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        eval_loss += loss.item()\n",
    "total_time = time.time() - start_time\n",
    "secs_per_image = total_time / (len(test_loader.dataset))\n",
    "\n",
    "results[name] = {\"test_loss\": eval_loss / len(test_loader), \"secs_per_image\": secs_per_image}\n",
    "\n",
    "# Report\n",
    "print(\"\\n=== Results ===\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(\n",
    "        f\"{model_name:15s} | Loss: {metrics['test_loss']:.4f} | Secs/img: {metrics['secs_per_image']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(4):\n",
    "        img, mask = test_loader.dataset[i]\n",
    "        # img, mask = test_loader.dataset[len(test_loader.dataset) - 1 - i]\n",
    "        img = img.to(device)\n",
    "        out = model(img.unsqueeze(0))\n",
    "        pred = out.argmax(1).squeeze().cpu()\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(img.permute(1, 2, 0).cpu())\n",
    "        plt.title(\"Input\")\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask.squeeze())\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred)\n",
    "        plt.title(\"Prediction\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811c1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0\n",
    "for i in range(100):\n",
    "    img, mask = test_loader.dataset[i]\n",
    "    # img, mask = test_loader.dataset[len(test_loader.dataset) - 1 - i]\n",
    "    img = img.to(device)\n",
    "    tic = time.time()\n",
    "    out = model(img.unsqueeze(0))\n",
    "    toc = time.time()\n",
    "    total_time += toc - tic\n",
    "    pred = out.argmax(1).squeeze().cpu()\n",
    "\n",
    "print(f\"{total_time / 100:.4f} seconds per image\")\n",
    "print(100 / total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65789b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "model_path = f\"model_{name}.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import cv2\n",
    "from lac.perception.segmentation import UnetSegmentation, SemanticClasses\n",
    "from lac.params import LAC_BASE_PATH\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376fc2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.perception.langsam import LangSAMSegmentation\n",
    "\n",
    "langsam = LangSAMSegmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63120a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "3 == SemanticClasses.GROUND.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b455274",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(LAC_BASE_PATH) / \"lunar_autonomy_challenge\" / \"models\" / \"model_UnetPlusPlus.pth\"\n",
    "model = UnetSegmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92378e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(LAC_BASE_PATH) / \"output/DataCollectionAgent/stereo_lights1.0_map1_preset1\"\n",
    "test_img = cv2.imread(str(data_path / \"FrontLeft\" / \"200.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(test_img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b27344",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_PIL = Image.fromarray(test_img).convert(\"RGB\")\n",
    "results, full_mask_langsam = langsam.segment_rocks(test_img_PIL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
