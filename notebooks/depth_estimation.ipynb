{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-Anything from HuggingFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n",
    "pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "\n",
    "image_path = os.path.expanduser(\n",
    "    \"~/LunarAutonomyChallenge/output/data_collection_1/front_left/{}.png\".format(i)\n",
    ")\n",
    "image = Image.open(image_path)\n",
    "\n",
    "predictions = pipe(image)\n",
    "\n",
    "# Plot image and predicted depth side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={\"wspace\": 0, \"hspace\": 0})\n",
    "axes[0].imshow(image, cmap=\"gray\")\n",
    "axes[1].imshow(predictions[\"depth\"], cmap=\"gray\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple Depth Pro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import depth_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and preprocessing transform\n",
    "model, transform = depth_pro.create_model_and_transforms(device=device)\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess an image.\n",
    "image, _, f_px = depth_pro.load_rgb(image_path)\n",
    "image = transform(image)\n",
    "\n",
    "# Run inference.\n",
    "prediction = model.infer(image, f_px=f_px)\n",
    "depth = prediction[\"depth\"]  # Depth in [m].\n",
    "focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth.cpu(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo\n",
    "\n",
    "- Stereo baseline = 0.162 m\n",
    "- All cameras horizontal FOV = 1.22 radians (70 degrees)\n",
    "\n",
    "\"The cameras are modelled as perfect pinhole cameras with square pixels, there is no lens distortion. Lens flare from the sun is modelled, this should be considered as a potential source of error in segmentation and feature detection. Each camera has the same field of view of 1.22 radians (70 degrees). The resolution is set by the agent upon initialization in the sensors() method. The maximum resolution allowed is 2448 x 2048 pixels, if a resolution higher than this is requested the resolution will be clipped to the maximum and a warning will be given on the command line.\"\n",
    "\n",
    "From discord:\n",
    "\"Effectively, there is no focal length because the simulator does not model a physical camera, it is modelled as a perfect pinhole camera. Normally, the focal length is given in mm and to relate a pixel coordinate to a line extending from the camera center into the world, you need to use the pixel dimensions in mm.\n",
    "\n",
    "The trick is to express the focal length in terms of pixels. Draw the camera geometry in a diagram, the focal length is the distance between the camera center and the image plane. You also know the width and height of the sensor in terms of pixels because you set this in your agent set up, you also know the FOV of the camera. Using triangular geometric relations you can express the focal length in terms of pixels.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = 1280, 720\n",
    "FOV = 1.22  # radians\n",
    "BASELINE = 0.162  # meters\n",
    "\n",
    "data_path = Path(\"../../output/data_collection_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 40\n",
    "\n",
    "left_image_path = data_path / \"front_left\" / f\"{i}.png\"\n",
    "right_image_path = data_path / \"front_right\" / f\"{i}.png\"\n",
    "left_image = Image.open(left_image_path)\n",
    "right_image = Image.open(data_path / \"front_right\" / f\"{i}.png\")\n",
    "\n",
    "# Plot image and predicted depth side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={\"wspace\": 0, \"hspace\": 0})\n",
    "axes[0].imshow(left_image, cmap=\"gray\")\n",
    "axes[1].imshow(right_image, cmap=\"gray\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.perception.depth import compute_stereo_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_length_x = W / (2 * np.tan(FOV / 2))\n",
    "focal_length_y = H / (2 * np.tan(FOV / 2))\n",
    "\n",
    "disparity, depth = compute_stereo_depth(\n",
    "    np.array(left_image), np.array(right_image), BASELINE, focal_length_x, semi_global=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(focal_length_x, focal_length_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={\"wspace\": 0, \"hspace\": 0})\n",
    "axes[0].imshow(disparity, cmap=\"gray\")\n",
    "axes[1].imshow(depth, cmap=\"gray\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo with segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heightmap reprojection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heightmap_path = \"../../data/heightmaps/Moon_Map_01_0_rep0.dat\"\n",
    "heightmap = np.load(heightmap_path, allow_pickle=True)\n",
    "heightmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_pose = np.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
