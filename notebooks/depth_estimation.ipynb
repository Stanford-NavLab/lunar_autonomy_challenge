{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import lac.params as params\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-Anything from HuggingFace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n",
    "pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "\n",
    "image_path = os.path.expanduser(\n",
    "    \"~/LunarAutonomyChallenge/output/data_collection_1/front_left/{}.png\".format(i)\n",
    ")\n",
    "image = Image.open(image_path)\n",
    "\n",
    "predictions = pipe(image)\n",
    "\n",
    "# Plot image and predicted depth side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={\"wspace\": 0, \"hspace\": 0})\n",
    "axes[0].imshow(image, cmap=\"gray\")\n",
    "axes[1].imshow(predictions[\"depth\"], cmap=\"gray\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple Depth Pro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import depth_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and preprocessing transform\n",
    "model, transform = depth_pro.create_model_and_transforms(device=device)\n",
    "model.eval()\n",
    "\n",
    "# Load and preprocess an image.\n",
    "image, _, f_px = depth_pro.load_rgb(image_path)\n",
    "image = transform(image)\n",
    "\n",
    "# Run inference.\n",
    "prediction = model.infer(image, f_px=f_px)\n",
    "depth = prediction[\"depth\"]  # Depth in [m].\n",
    "focallength_px = prediction[\"focallength_px\"]  # Focal length in pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth.cpu(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo\n",
    "\n",
    "- Stereo baseline = 0.162 m\n",
    "- All cameras horizontal FOV = 1.22 radians (70 degrees)\n",
    "\n",
    "\"The cameras are modelled as perfect pinhole cameras with square pixels, there is no lens distortion. Lens flare from the sun is modelled, this should be considered as a potential source of error in segmentation and feature detection. Each camera has the same field of view of 1.22 radians (70 degrees). The resolution is set by the agent upon initialization in the sensors() method. The maximum resolution allowed is 2448 x 2048 pixels, if a resolution higher than this is requested the resolution will be clipped to the maximum and a warning will be given on the command line.\"\n",
    "\n",
    "From discord:\n",
    "\"Effectively, there is no focal length because the simulator does not model a physical camera, it is modelled as a perfect pinhole camera. Normally, the focal length is given in mm and to relate a pixel coordinate to a line extending from the camera center into the world, you need to use the pixel dimensions in mm.\n",
    "\n",
    "The trick is to express the focal length in terms of pixels. Draw the camera geometry in a diagram, the focal length is the distance between the camera center and the image plane. You also know the width and height of the sensor in terms of pixels because you set this in your agent set up, you also know the FOV of the camera. Using triangular geometric relations you can express the focal length in terms of pixels.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = 1280, 720\n",
    "FOV = 1.22  # radians\n",
    "BASELINE = 0.162  # meters\n",
    "\n",
    "data_path = Path(\"../../output/data_collection_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 40\n",
    "\n",
    "left_image_path = data_path / \"front_left\" / f\"{i}.png\"\n",
    "right_image_path = data_path / \"front_right\" / f\"{i}.png\"\n",
    "left_image = Image.open(left_image_path)\n",
    "right_image = Image.open(data_path / \"front_right\" / f\"{i}.png\")\n",
    "\n",
    "# Plot image and predicted depth side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={\"wspace\": 0, \"hspace\": 0})\n",
    "axes[0].imshow(left_image, cmap=\"gray\")\n",
    "axes[1].imshow(right_image, cmap=\"gray\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.perception.depth import compute_stereo_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_length_x = W / (2 * np.tan(FOV / 2))\n",
    "focal_length_y = H / (2 * np.tan(FOV / 2))\n",
    "\n",
    "disparity, depth = compute_stereo_depth(\n",
    "    np.array(left_image), np.array(right_image), BASELINE, focal_length_x, semi_global=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(focal_length_x, focal_length_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={\"wspace\": 0, \"hspace\": 0})\n",
    "axes[0].imshow(disparity, cmap=\"gray\")\n",
    "axes[1].imshow(depth, cmap=\"gray\")\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo with segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.util import load_data\n",
    "from lac.utils.visualization import overlay_mask\n",
    "from lac.perception.segmentation import Segmentation\n",
    "from lac.perception.vision import project_pixel_to_3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation = Segmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../../output/nav_agent_preset_2_seed_4\")\n",
    "initial_pose, lander_pose, poses, imu_data, cam_config = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1020\n",
    "\n",
    "left_image = Image.open(data_path / \"FrontLeft\" / f\"{i}.png\")\n",
    "right_image = Image.open(data_path / \"FrontRight\" / f\"{i}.png\")\n",
    "\n",
    "left_seg_results, left_seg_mask = segmentation.segment_rocks(left_image.convert(\"RGB\"))\n",
    "right_seg_results, right_seg_mask = segmentation.segment_rocks(right_image.convert(\"RGB\"))\n",
    "\n",
    "left_overlay = overlay_mask(np.array(left_image), left_seg_mask)\n",
    "right_overlay = overlay_mask(np.array(right_image), right_seg_mask)\n",
    "\n",
    "# Plot image and predicted depth side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10), gridspec_kw={\"wspace\": 0, \"hspace\": 0})\n",
    "axes[0].imshow(left_overlay)\n",
    "axes[1].imshow(right_overlay)\n",
    "for ax in axes:\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.perception.depth import stereo_depth_from_segmentation, project_depths_to_world\n",
    "from lac.utils.visualization import overlay_stereo_rock_depths\n",
    "\n",
    "results = stereo_depth_from_segmentation(\n",
    "    left_seg_results, right_seg_results, params.STEREO_BASELINE, params.FL_X\n",
    ")\n",
    "left_overlay = overlay_stereo_rock_depths(left_overlay, results)\n",
    "plt.imshow(left_overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    point = project_pixel_to_3D(result[\"left_centroid\"], result[\"depth\"], params.CAMERA_INTRINSICS)\n",
    "    print(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rover_pose = poses[i]\n",
    "\n",
    "project_depths_to_world(results, rover_pose, cam_name=\"FrontLeft\", cam_config=cam_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a matching method based on the masks themselves, i.e., for each predicted mask in left image,\n",
    "search for the most similar mask in the right image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heightmap reprojection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heightmap_path = \"../../data/heightmaps/Moon_Map_01_0_rep0.dat\"\n",
    "heightmap = np.load(heightmap_path, allow_pickle=True)\n",
    "heightmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_pose = np.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
