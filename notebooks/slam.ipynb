{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual SLAM\n",
    "\n",
    "1. Initialization: At first frame, initialize map with 3D points from stereo.\n",
    "2. Tracking:\n",
    "   - at frame i+1, match keypoints between i and i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from lightglue import LightGlue, SuperPoint\n",
    "from lightglue.utils import rbd\n",
    "\n",
    "from lac.perception.vision import LightGlueMatcher\n",
    "from lac.perception.depth import project_pixel_to_rover\n",
    "from lac.utils.frames import apply_transform\n",
    "from lac.utils.plotting import plot_3d_points, plot_surface, plot_poses\n",
    "from lac.util import load_data, grayscale_to_3ch_tensor\n",
    "import lac.params as params\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\n",
    "    os.path.expanduser(\"~/LunarAutonomyChallenge/output/NavAgent/map1_preset4_gtnav_steer\")\n",
    ")\n",
    "initial_pose, lander_pose, poses, imu_data, cam_config = load_data(data_path)\n",
    "print(f\"Num poses: {len(poses)}\")\n",
    "\n",
    "map = np.load(\"../../data/heightmaps/competition/Moon_Map_01_preset_0.dat\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the images\n",
    "\n",
    "left_imgs = {}\n",
    "right_imgs = {}\n",
    "\n",
    "for img_name in os.listdir(data_path / \"FrontLeft\"):\n",
    "    left_imgs[int(img_name.split(\".\")[0])] = cv2.imread(\n",
    "        str(data_path / \"FrontLeft\" / img_name), cv2.IMREAD_GRAYSCALE\n",
    "    )\n",
    "\n",
    "for img_name in os.listdir(data_path / \"FrontRight\"):\n",
    "    right_imgs[int(img_name.split(\".\")[0])] = cv2.imread(\n",
    "        str(data_path / \"FrontRight\" / img_name), cv2.IMREAD_GRAYSCALE\n",
    "    )\n",
    "\n",
    "assert len(left_imgs.keys()) == len(right_imgs.keys())\n",
    "img_idxs = sorted(left_imgs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_surface(map)\n",
    "fig = plot_poses(poses[::20], fig=fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = SuperPoint(max_num_keypoints=2048).eval().cuda()  # load the extractor\n",
    "matcher = LightGlue(features=\"superpoint\").eval().cuda()  # load the matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stereo (PnP) VO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.localization.slam.visual_odometry import StereoVisualOdometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo = StereoVisualOdometry(cam_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 80\n",
    "\n",
    "svo.initialize(poses[start_idx], left_imgs[start_idx], right_imgs[start_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo_poses = [poses[start_idx]]\n",
    "\n",
    "idx = start_idx\n",
    "\n",
    "for i in tqdm(range(1980)):\n",
    "    idx += 2\n",
    "    svo.track(left_imgs[idx], right_imgs[idx])\n",
    "    svo_poses.append(svo.rover_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_poses(poses, no_axes=True, color=\"black\", name=\"Ground Truth\")\n",
    "fig = plot_poses(svo_poses, no_axes=True, fig=fig, color=\"red\", name=\"Stereo (PnP) VO\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"stereo_pnp_vo.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 80\n",
    "\n",
    "feats0_left = extractor.extract(grayscale_to_3ch_tensor(left_imgs[idx]).cuda())\n",
    "feats0_right = extractor.extract(grayscale_to_3ch_tensor(right_imgs[idx]).cuda())\n",
    "\n",
    "matches0_stereo = matcher({\"image0\": feats0_left, \"image1\": feats0_right})\n",
    "matches0_stereo = rbd(matches0_stereo)[\"matches\"]  # indices with shape (K,2)\n",
    "points0_left = rbd(feats0_left)[\"keypoints\"][\n",
    "    matches0_stereo[..., 0]\n",
    "]  # coordinates in image #0, shape (K,2)\n",
    "points0_right = rbd(feats0_right)[\"keypoints\"][\n",
    "    matches0_stereo[..., 1]\n",
    "]  # coordinates in image #1, shape (K,2)\n",
    "\n",
    "disparities = (points0_left - points0_right)[:, 0]\n",
    "depths = params.FL_X * params.STEREO_BASELINE / disparities\n",
    "\n",
    "points0_left = points0_left.cpu().numpy()\n",
    "depths = depths.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points0_rover = []\n",
    "for pixel, depth in zip(points0_left, depths):\n",
    "    point_rover = project_pixel_to_rover(pixel, depth, \"FrontLeft\", cam_config)\n",
    "    points0_rover.append(point_rover)\n",
    "points0_rover = np.array(points0_rover)\n",
    "points0_world = apply_transform(poses[idx], points0_rover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1_left = extractor.extract(grayscale_to_3ch_tensor(left_imgs[idx + 2]).cuda())\n",
    "\n",
    "matches01_left = matcher({\"image0\": feats0_left, \"image1\": feats1_left})\n",
    "matches01_left = rbd(matches01_left)[\"matches\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract indices from the first column (img0_left indices)\n",
    "stereo_indices = matches0_stereo[:, 0]\n",
    "frame_indices = matches01_left[:, 0]\n",
    "\n",
    "# Find the intersection of indices\n",
    "common_indices = torch.tensor(\n",
    "    list(set(stereo_indices.cpu().numpy()) & set(frame_indices.cpu().numpy()))\n",
    ").cuda()\n",
    "\n",
    "# Get the matches from both tensors corresponding to the common indices\n",
    "stereo_common = matches0_stereo[torch.isin(stereo_indices, common_indices)]\n",
    "frame_common = matches01_left[torch.isin(frame_indices, common_indices)]\n",
    "points0_world_common = points0_world[torch.isin(stereo_indices, common_indices).cpu().numpy()]\n",
    "\n",
    "print(f\"Number of common matches: {len(common_indices)}\")\n",
    "# print(\"Stereo Matches:\", stereo_common)\n",
    "# print(\"Frame-to-Frame Matches:\", frame_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points3D = points0_world_common\n",
    "points2D = rbd(feats1_left)[\"keypoints\"][frame_common[:, 1]].cpu().numpy()\n",
    "print(points3D.shape, points2D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.utils.frames import invert_transform_mat, OPENCV_TO_CAMERA_PASSIVE, get_cam_pose_rover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera intrinsics (K)\n",
    "K = params.CAMERA_INTRINSICS\n",
    "\n",
    "# Estimate relative motion with PnP (RANSAC for robustness)\n",
    "success, rvec, tvec, inliers = cv2.solvePnPRansac(\n",
    "    objectPoints=points3D,\n",
    "    imagePoints=points2D,\n",
    "    cameraMatrix=K,\n",
    "    distCoeffs=None,\n",
    "    flags=cv2.SOLVEPNP_ITERATIVE,\n",
    "    reprojectionError=8.0,\n",
    "    iterationsCount=100,\n",
    ")\n",
    "\n",
    "if success:\n",
    "    # Convert rotation vector to rotation matrix\n",
    "    R, _ = cv2.Rodrigues(rvec)\n",
    "    T = np.hstack((R, tvec))  # [R | t]\n",
    "    est_pose = np.vstack((T, [0, 0, 0, 1]))  # Homogeneous transform (4x4)\n",
    "    w_T_c = invert_transform_mat(est_pose)  # world to opencv passive\n",
    "    w_T_c[:3, :3] = w_T_c[:3, :3] @ OPENCV_TO_CAMERA_PASSIVE  # world to camera passive\n",
    "    rover_to_cam = get_cam_pose_rover(\"FrontLeft\")\n",
    "    cam_to_rover = invert_transform_mat(rover_to_cam)\n",
    "    rover_pose = w_T_c @ cam_to_rover\n",
    "\n",
    "    print(\"Estimated pose at frame 82:\")\n",
    "    print(rover_pose)\n",
    "else:\n",
    "    print(\"PnP failed to estimate motion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = LightGlueMatcher()\n",
    "\n",
    "feats0, feats1, matches01 = matcher.match(left_imgs[2], right_imgs[2])\n",
    "matches = matches01[\"matches\"]  # indices with shape (K,2)\n",
    "points0 = feats0[\"keypoints\"][matches[..., 0]]  # coordinates in image #0, shape (K,2)\n",
    "points1 = feats1[\"keypoints\"][matches[..., 1]]  # coordinates in image #1, shape (K,2)\n",
    "\n",
    "disparities = (points0 - points1)[:, 0]\n",
    "depths = params.FL_X * params.STEREO_BASELINE / disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stereo_to_rover_points(left_img, right_img):\n",
    "    feats0, feats1, matches01 = matcher.match(left_img, right_img)\n",
    "    matches = matches01[\"matches\"]  # indices with shape (K,2)\n",
    "    points0 = feats0[\"keypoints\"][matches[..., 0]]  # coordinates in image #0, shape (K,2)\n",
    "    points1 = feats1[\"keypoints\"][matches[..., 1]]  # coordinates in image #1, shape (K,2)\n",
    "    descriptors0 = feats0[\"descriptors\"][matches[..., 0]]\n",
    "    descriptors1 = feats1[\"descriptors\"][matches[..., 1]]\n",
    "\n",
    "    disparities = (points0 - points1)[:, 0]\n",
    "    depths = params.FL_X * params.STEREO_BASELINE / disparities\n",
    "\n",
    "    points_rover = []\n",
    "    for pixel, depth in zip(points0, depths):\n",
    "        point_rover = project_pixel_to_rover(pixel, depth, \"FrontLeft\", cam_config)\n",
    "        points_rover.append(point_rover)\n",
    "    return np.array(points_rover), descriptors0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points, descriptors = stereo_to_rover_points(left_imgs[80], right_imgs[80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points_world = []\n",
    "\n",
    "for i in np.arange(100, 2000, 100):\n",
    "    points_rover = stereo_to_rover_points(left_imgs[i], right_imgs[i])\n",
    "    points_world = apply_transform(poses[i], points_rover)\n",
    "    all_points_world.append(points_world)\n",
    "\n",
    "all_points_world = np.concatenate(all_points_world, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_surface(map)\n",
    "# fig = plot_poses(poses[::20], fig=fig)\n",
    "fig = plot_3d_points(all_points_world, fig=fig)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MapPoint:\n",
    "    xyz: np.ndarray\n",
    "    descriptor: np.ndarray\n",
    "    label: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SLAM:\n",
    "    def __init__(self):\n",
    "        self.matcher = LightGlueMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtsam\n",
    "from gtsam.symbol_shorthand import B, V, X, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 1.622\n",
    "IMU_PARAMS = gtsam.PreintegrationParams.MakeSharedU(g)\n",
    "I = np.eye(3)\n",
    "IMU_PARAMS.setAccelerometerCovariance(I * 0.2)\n",
    "IMU_PARAMS.setGyroscopeCovariance(I * 0.2)\n",
    "IMU_PARAMS.setIntegrationCovariance(I * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = gtsam.NonlinearFactorGraph()\n",
    "initial_estimate = gtsam.Values()\n",
    "\n",
    "pose_key = X(0)\n",
    "pose_noise = gtsam.noiseModel.Diagonal.Sigmas(0.01 * np.ones(6))\n",
    "pose_0 = gtsam.Pose3(poses[0])\n",
    "graph.push_back(gtsam.PriorFactorPose3(pose_key, pose_0, pose_noise))\n",
    "initial_estimate.insert(pose_key, pose_0)\n",
    "\n",
    "velocity_key = V(0)\n",
    "velocity_noise = gtsam.noiseModel.Isotropic.Sigma(3, 0.1)\n",
    "velocity_0 = np.zeros(3)\n",
    "graph.push_back(gtsam.PriorFactorVector(velocity_key, velocity_0, velocity_noise))\n",
    "\n",
    "# Preintegrator\n",
    "accum = gtsam.PreintegratedImuMeasurements(IMU_PARAMS)\n",
    "\n",
    "n_frames = 100\n",
    "\n",
    "for i in range(0, n_frames):\n",
    "    accum.integrateMeasurement(imu_data[i, :3], imu_data[i, 3:], params.DT)\n",
    "    pose_key += 1\n",
    "    DELTA = gtsam.Pose3(\n",
    "        gtsam.Rot3.Rodrigues(0, 0, 0.1 * np.random.randn()),\n",
    "        gtsam.Point3(4 * np.random.randn(), 4 * np.random.randn(), 4 * np.random.randn()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
