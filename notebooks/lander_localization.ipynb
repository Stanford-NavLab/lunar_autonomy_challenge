{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from lang_sam import LangSAM\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from lac.localization.ekf import EKF\n",
    "import typing as T\n",
    "from plotly import graph_objects as go\n",
    "from lac.plotting import pose_trace, vector_trace\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an image containing the lander, generate range and bearing measurements to the lander.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define camera intrinsics\n",
    "W, H = 1280, 720\n",
    "FOV = 1.22  # radians\n",
    "focal_length_x = W / (2 * np.tan(FOV / 2))\n",
    "focal_length_y = H / (2 * np.tan(FOV / 2))\n",
    "K = np.array([[focal_length_x / 2, 0, W / 2], [0, focal_length_y / 2, H / 2], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---- TEST IMAGES FOR FIDUCIAL SHAPE SEG -------------------------------\n",
    "# data_path = os.path.expanduser(\"~/LunarAutonomyChallenge/output/data_collection_1\")\n",
    "\n",
    "# data_collection_1\n",
    "# true examples\n",
    "# /home/lac/LunarAutonomyChallenge/output/data_collection_1/front_left/35.png # group D\n",
    "# /home/lac/LunarAutonomyChallenge/output/data_collection_1/front_left/124.png # group D\n",
    "# /home/lac/LunarAutonomyChallenge/output/data_collection_1/front_left/1527.png # group A\n",
    "# /home/lac/LunarAutonomyChallenge/output/data_collection_1/front_left/1620.png # group A\n",
    "\n",
    "# false examples\n",
    "# /home/lac/LunarAutonomyChallenge/output/data_collection_1/front_left/328.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/data_collection_1/front_left/619.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/data_collection_1/front_left/974.png\n",
    "\n",
    "# lander_closeups\n",
    "# true examples\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/53.png\n",
    "\n",
    "# i = [35, 124, 328, 619, 974, 1527, 1620]\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# for ind in i:\n",
    "#     I1 = cv.imread(os.path.join(data_path, \"FrontLeft\", f\"{ind}.png\"), cv.IMREAD_GRAYSCALE)\n",
    "#     plt.imshow(I1, cmap=\"gray\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without fiducials\n",
    "\n",
    "Use segmentation to identify the lander outline. Next, identify extreme points on the lander outline.\n",
    "Assuming that we know the lander geometry, we know the location of these points in 3D relative to the\n",
    "lander center.\n",
    "\n",
    "Alternatively, we can try to directly predict the lander center (in the image in image coordinates)\n",
    "from the segmentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.expanduser(\"~/LunarAutonomyChallenge/output/lander_no_fiducials_left_far/\")\n",
    "CAM_NAME = \"Left\"\n",
    "imgs = os.listdir(os.path.join(data_path, CAM_NAME))\n",
    "imgs = [os.path.join(data_path, CAM_NAME, imgs[i]) for i in range(0, len(imgs), 1)]\n",
    "imgs.sort()\n",
    "\n",
    "# # Segment lander\n",
    "# model = LangSAM()\n",
    "# text_prompt = \"lander.\"\n",
    "# SEG_SCORE_THRESH = 0.4 # requires more testing\n",
    "\n",
    "# for img in imgs:\n",
    "#     I1 = cv.imread(os.path.join(data_path, CAM_NAME, img), cv.IMREAD_GRAYSCALE)\n",
    "#     plt.imshow(I1, cmap=\"gray\")\n",
    "#     plt.show()\n",
    "\n",
    "#     image_seg_in = Image.open(os.path.join(data_path, CAM_NAME, img)).convert(\"RGB\")\n",
    "#     results = model.predict([image_seg_in], [text_prompt])\n",
    "#     full_mask = np.zeros_like(image_seg_in).copy()\n",
    "#     if len(results) == 0:\n",
    "#         print(\"No masks found\")\n",
    "#     for i, mask in enumerate(results[0][\"masks\"]):\n",
    "#         print(results[0][\"scores\"][i])\n",
    "#         if results[0][\"scores\"][i] < SEG_SCORE_THRESH:\n",
    "#             continue\n",
    "#         full_mask[mask.astype(bool)] = 255\n",
    "\n",
    "#         # convert mask to pixel coordinates to get min area rect\n",
    "#         mask = mask.astype(np.uint8)\n",
    "#         contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "#         if contours:\n",
    "#             largest_contour = max(contours, key=cv.contourArea)  # Pick the biggest object\n",
    "#             rect = cv.minAreaRect(largest_contour)\n",
    "#             box = cv.boxPoints(rect)\n",
    "#             box = np.intp(box)\n",
    "#             cv.drawContours(full_mask, [box], 0, (0, 0, 255), 2)\n",
    "\n",
    "#             # sort the box points by the y coordinate\n",
    "#             box = box[np.argsort(box[:, 1])]\n",
    "#             lower_corners = box[2:]\n",
    "#             mid_point = np.mean(lower_corners, axis=0)\n",
    "#             cv.circle(full_mask, tuple(mid_point.astype(int)), 10, (0, 0, 255), -1)\n",
    "\n",
    "#         # ------ PRIOR ATTEMPTS TO GET CENTER POINT BETWEEN LANDER FEET ------\n",
    "#         # # get bounding box\n",
    "#         # x, y, w, h = cv.boundingRect(mask)\n",
    "#         # cv.rectangle(full_mask, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "#         # x_avg = int(x + w/2)\n",
    "#         # y_max = int(y + h)\n",
    "#         # cv.circle(full_mask, (x_avg, y_max), 10, (0, 255, 0), -1)\n",
    "\n",
    "#         # # get average x and max y of the mask to get the center point\n",
    "#         # mask_x_avg = int(np.mean(np.where(mask)[1]))\n",
    "#         # mask_y_max = int(np.max(np.where(mask)[0]))\n",
    "#         # cv.circle(full_mask, (mask_x_avg, mask_y_max), 10, (255, 0, 0), -1)\n",
    "\n",
    "#     plt.imshow(full_mask, cmap=\"gray\")\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roteuler_tvec_from_poselist(poses: T.Union[T.List[np.ndarray], np.ndarray]) -> tuple:\n",
    "    \"\"\"\n",
    "    Get Euler angles and translation vectors from poses. 4x4 for a single pose, Nx4x4 for a list of poses.\n",
    "    Input:\n",
    "        poses - list of poses or a single pose\n",
    "    Output:\n",
    "        Returns a tuple of Euler angles and tvec (lists if input is a list of poses)\n",
    "    \"\"\"\n",
    "\n",
    "    def pose_to_roteuler_tvec(pose: np.ndarray) -> tuple:\n",
    "        \"\"\"Get rpy and tvec from a single pose\"\"\"\n",
    "        r = R.from_matrix(pose[:3, :3])\n",
    "        rpy = r.as_euler(\"xyz\", degrees=False)\n",
    "        tvec = pose[:3, 3]\n",
    "        return rpy, tvec\n",
    "\n",
    "    if len(poses.shape) == 3 or isinstance(poses, list):  # list of poses\n",
    "        rpy, tvec = [], []\n",
    "        for pose in poses:\n",
    "            r, t = pose_to_roteuler_tvec(pose)\n",
    "            rpy.append(r)\n",
    "            tvec.append(t)\n",
    "        return rpy, tvec\n",
    "\n",
    "    rpy, tvec = pose_to_roteuler_tvec(poses)\n",
    "    return rpy, tvec\n",
    "\n",
    "\n",
    "class MeasModelLanderSeg:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        lander_pose: np.ndarray,\n",
    "        cam_pose: np.ndarray,\n",
    "        seg_thresh: float = 0.4,\n",
    "        K: np.ndarray = K,\n",
    "        R_nom: np.ndarray = np.eye(3) * 0.01,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.seg_thresh = seg_thresh\n",
    "        self.cam_rpy_rover, self.cam_pos_rover = get_roteuler_tvec_from_poselist(\n",
    "            cam_pose\n",
    "        )  # camera position relative to rover in rover frame\n",
    "        self.lander_rpy_world, self.lander_pos_world = get_roteuler_tvec_from_poselist(\n",
    "            lander_pose\n",
    "        )  # w.r.t. world frame\n",
    "        self.K = K  # camera intrinsics\n",
    "        self.R = R_nom  # measurement noise covariance matrix under nominal conditions\n",
    "\n",
    "    def get_lander_origin_pix_from_image(self, img_path: str, display: bool = False) -> np.ndarray:\n",
    "        \"\"\"Measurement function for extracting the lander origin pixel coordinates from an image\"\"\"\n",
    "\n",
    "        lander_origin = None\n",
    "\n",
    "        # Segment lander\n",
    "        image_seg_in = Image.open(img_path).convert(\"RGB\")\n",
    "        text_prompt = \"lander.\"\n",
    "        results = self.model.predict([image_seg_in], [text_prompt])\n",
    "        full_mask = np.zeros_like(image_seg_in).copy()\n",
    "\n",
    "        if len(results) == 0:  # no masks found\n",
    "            return lander_origin\n",
    "\n",
    "        for i, mask in enumerate(results[0][\"masks\"]):\n",
    "            if results[0][\"scores\"][i] < self.seg_thresh:\n",
    "                continue\n",
    "\n",
    "            full_mask[mask.astype(bool)] = 255\n",
    "            mask = mask.astype(np.uint8)\n",
    "            contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            if contours:\n",
    "                largest_contour = max(contours, key=cv.contourArea)\n",
    "                rect = cv.minAreaRect(largest_contour)\n",
    "                box = cv.boxPoints(rect)\n",
    "                box = np.intp(box)\n",
    "\n",
    "                # sort the box points by the y coordinate to get the lower corners (lander feet)\n",
    "                sorted_box = box[np.argsort(box[:, 1])]\n",
    "                lower_corners = sorted_box[2:]\n",
    "                mid_point = np.mean(lower_corners, axis=0)\n",
    "                lander_origin = mid_point\n",
    "\n",
    "                if display:\n",
    "                    cv.drawContours(full_mask, [box], 0, (0, 0, 255), 2)\n",
    "                    cv.circle(full_mask, tuple(mid_point.astype(int)), 10, (0, 0, 255), -1)\n",
    "\n",
    "        if display:\n",
    "            plt.imshow(full_mask, cmap=\"gray\")\n",
    "            plt.show()\n",
    "\n",
    "        return lander_origin\n",
    "\n",
    "    def get_lander_origin_pix_from_proj(\n",
    "        self, rover_state: np.ndarray, display: bool = False\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the lander origin pixel coordinates via projection to the image plane.\n",
    "        NOTE: X_rot_Y is the rotation matrix that rotates the Y frame to the X frame.\n",
    "        NOTE: rpy is roll, pitch, yaw in radians.\n",
    "\n",
    "        Input:\n",
    "            rover_state - state of the rover w.r.t. the world frame\n",
    "        Output:\n",
    "            lander_origin_pix - lander origin pixel coordinates [u, v]\n",
    "        \"\"\"\n",
    "\n",
    "        OPENCVCAM_ROT_ROVER = np.array([[0, -1, 0], [0, 0, -1], [1, 0, 0]])\n",
    "\n",
    "        rover_pos_world = rover_state[:3]\n",
    "        rover_rpy_world = rover_state[6:]\n",
    "\n",
    "        rover_rot_world = R.from_euler(\"xyz\", rover_rpy_world, degrees=False).as_matrix()\n",
    "        cam_rot_rover = R.from_euler(\"xyz\", self.cam_rpy_rover, degrees=False).as_matrix()\n",
    "        cam_rot_world = cam_rot_rover @ rover_rot_world\n",
    "        opencvcam_rot_cam = OPENCVCAM_ROT_ROVER @ cam_rot_rover.T\n",
    "\n",
    "        # print(\"RPY world to rover: \", R.from_matrix(rover_rot_world).as_euler(\"xyz\", degrees=True))\n",
    "        # print(\"RPY rover to cam: \", R.from_matrix(cam_rot_rover).as_euler(\"xyz\", degrees=True))\n",
    "        # print(\"RPY world to cam: \", R.from_matrix(cam_rot_world).as_euler(\"xyz\", degrees=True))\n",
    "\n",
    "        cam_pos_rover_world = rover_rot_world.T @ self.cam_pos_rover\n",
    "        cam_pos_world = cam_pos_rover_world + rover_pos_world\n",
    "        lander_pos_cam = opencvcam_rot_cam @ cam_rot_world @ (self.lander_pos_world - cam_pos_world)\n",
    "        pix_coords = self.K @ lander_pos_cam\n",
    "\n",
    "        if pix_coords[2] > 0:  # Check if the point is in front of the camera\n",
    "            lander_origin_pix = pix_coords[:2] / pix_coords[2]\n",
    "        else:\n",
    "            print(\"Lander is behind the camera.\")\n",
    "\n",
    "        if display:\n",
    "            # # plot the rover and camera axes in the rover frame\n",
    "            # fig_rover = go.Figure()\n",
    "            # rover_ref_frame = pose_trace((np.eye(3), np.zeros(3)))\n",
    "            # cam_traces_rover = pose_trace((cam_rot_rover, self.cam_pos_rover))\n",
    "            # cam_pos_rover_vectrace = vector_trace(np.zeros(3), self.cam_pos_rover, color=\"orange\", name=\"cam_pos_rover\")\n",
    "\n",
    "            # for trace in rover_ref_frame:\n",
    "            #     fig_rover.add_trace(trace)\n",
    "            # for trace in cam_traces_rover:\n",
    "            #     fig_rover.add_trace(trace)\n",
    "            # for trace in cam_pos_rover_vectrace:\n",
    "            #     fig_rover.add_trace(trace)\n",
    "            # fig_rover.update_layout(height=700, width=1200, scene_aspectmode=\"data\")\n",
    "            # fig_rover.show()\n",
    "\n",
    "            # plot the rover and camera axes in the world frame\n",
    "            fig_world = go.Figure()\n",
    "\n",
    "            lander_traces = pose_trace(\n",
    "                (\n",
    "                    R.from_euler(\"xyz\", self.lander_rpy_world, degrees=False).as_matrix(),\n",
    "                    self.lander_pos_world,\n",
    "                )\n",
    "            )\n",
    "            rover_traces = pose_trace((rover_rot_world, rover_pos_world))\n",
    "            cam_traces = pose_trace((cam_rot_world, cam_pos_world))\n",
    "\n",
    "            opencv_traces = pose_trace((OPENCVCAM_ROT_ROVER @ rover_rot_world, cam_pos_world))\n",
    "\n",
    "            rover_pos_world_vectrace = vector_trace(\n",
    "                np.zeros(3),\n",
    "                rover_pos_world,\n",
    "                color=\"blue\",\n",
    "                name=\"[k] rover_pos w.r.t. world in world\",\n",
    "            )\n",
    "            lander_pos_world_vectrace = vector_trace(\n",
    "                np.zeros(3),\n",
    "                self.lander_pos_world,\n",
    "                color=\"purple\",\n",
    "                name=\"[k] lander_pos w.r.t. world in world\",\n",
    "            )\n",
    "\n",
    "            cam_pos_world_vectrace = vector_trace(\n",
    "                np.zeros(3), cam_pos_world, color=\"green\", name=\"[c] cam_pos w.r.t. world in world\"\n",
    "            )\n",
    "\n",
    "            cam_pos_rover_world_vectrace = vector_trace(\n",
    "                rover_pos_world,\n",
    "                rover_pos_world + cam_pos_rover_world,\n",
    "                color=\"orange\",\n",
    "                name=\"[c] cam_pos w.r.t. rover in world\",\n",
    "                head_size=0.1,\n",
    "            )\n",
    "\n",
    "            los_vec_world = self.lander_pos_world - cam_pos_world\n",
    "            los_vec_world_vectrace = vector_trace(\n",
    "                cam_pos_world,\n",
    "                cam_pos_world + los_vec_world,\n",
    "                color=\"red\",\n",
    "                name=\"[c] LOS Cam to Lander in world\",\n",
    "            )\n",
    "\n",
    "            for trace in lander_traces:\n",
    "                fig_world.add_trace(trace)\n",
    "            for trace in rover_traces:\n",
    "                fig_world.add_trace(trace)\n",
    "            for trace in cam_traces:\n",
    "                fig_world.add_trace(trace)\n",
    "            for trace in opencv_traces:\n",
    "                fig_world.add_trace(trace)\n",
    "            for trace in lander_pos_world_vectrace:\n",
    "                fig_world.add_trace(trace)\n",
    "            for trace in rover_pos_world_vectrace:\n",
    "                fig_world.add_trace(trace)\n",
    "            for trace in cam_pos_world_vectrace:\n",
    "                fig_world.add_trace(trace)\n",
    "            for trace in cam_pos_rover_world_vectrace:\n",
    "                fig_world.add_trace(trace)\n",
    "            for trace in los_vec_world_vectrace:\n",
    "                fig_world.add_trace(trace)\n",
    "\n",
    "            fig_world.update_layout(height=700, width=1200, scene_aspectmode=\"data\")\n",
    "            fig_world.show()\n",
    "\n",
    "        return lander_origin_pix[0:2]\n",
    "\n",
    "    def meas(self, rover_state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Measurement function for the EKF.\n",
    "        Input:\n",
    "            state - state vector of the EKF\n",
    "        Output:\n",
    "            z - measurement vector\n",
    "            H - measurement matrix\n",
    "            R - measurement noise covariance matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # get the lander origin pixel coordinates via projection to the image plane\n",
    "        lander_orig_pix = self.get_lander_origin_pix_from_proj(rover_state)\n",
    "\n",
    "        # get the Jacobian\n",
    "        H = np.zeros((2, len(rover_state)))\n",
    "\n",
    "        # get the measurement noise covariance matrix\n",
    "        # TODO: tune based on segmentation results? like if not the entire lander is visible in the image etc.\n",
    "\n",
    "        return lander_orig_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth trajectory data loading\n",
    "traj_data = json.load(open(f\"{data_path}/data_log.json\"))\n",
    "initial_pose = np.array(traj_data[\"initial_pose\"])\n",
    "data_log_path = os.path.join(data_path, \"data_log.json\")\n",
    "data_log = json.load(open(data_log_path))\n",
    "\n",
    "# Lander known pose w.r.t. world frame\n",
    "lander_pose_world = np.array(data_log[\"lander_pose_world\"])\n",
    "\n",
    "# Camera (front left) known pose w.r.t. rover body-fixed frame\n",
    "CAM_POS_KEYS = [\"x\", \"y\", \"z\"]\n",
    "CAM_ANG_KEYS = [\"roll\", \"pitch\", \"yaw\"]\n",
    "cams_geoms = json.load(open(os.path.expanduser(\"~/LunarAutonomyChallenge/docs/geometry.json\")))[\n",
    "    \"rover\"\n",
    "][\"cameras\"]\n",
    "cam = cams_geoms[CAM_NAME.lower()]\n",
    "rover_t_cam = np.array([cam[key] for key in CAM_POS_KEYS])\n",
    "rover_r_cam = R.from_euler(\"xyz\", [cam[key] for key in CAM_ANG_KEYS], degrees=False).as_matrix()\n",
    "cam_pose = np.eye(4)\n",
    "cam_pose[:3, :3] = rover_r_cam\n",
    "cam_pose[:3, 3] = rover_t_cam\n",
    "\n",
    "# Rover trajectory w.r.t. world frame\n",
    "ROVER_TRAJ_KEYS = [\"timestamp\", \"pose\"]\n",
    "frame_data = data_log[\"frames\"]\n",
    "gt_rover_traj_data = {key: [] for key in ROVER_TRAJ_KEYS}\n",
    "for frame in frame_data:\n",
    "    gt_rover_traj_data[\"timestamp\"].append(frame[\"timestamp\"])\n",
    "    gt_rover_traj_data[\"pose\"].append(frame[\"pose\"])\n",
    "gt_rover_traj_data[\"pose\"] = np.array(gt_rover_traj_data[\"pose\"])\n",
    "gt_rover_roteulers, gt_rover_tvecs = get_roteuler_tvec_from_poselist(gt_rover_traj_data[\"pose\"])\n",
    "dt = np.diff(gt_rover_traj_data[\"timestamp\"])\n",
    "\n",
    "# # visualize data\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.plot(lander_pose_world[0, 3], lander_pose_world[1, 3], \"ro\", label=\"lander\")\n",
    "# plt.plot(gt_rover_traj_data[\"pose\"][:, 0, 3], gt_rover_traj_data[\"pose\"][:, 1, 3], \"b-\", label=\"rover\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract IMU measurements\n",
    "imu_meas = []  # list of 6D IMU measurements (linear acceleration, angular velocity)\n",
    "for frame in frame_data:\n",
    "    imu_meas.append(np.array(frame[\"imu\"], dtype=np.float64))\n",
    "\n",
    "# extract camera measurements\n",
    "lander_orig_pix_meas = []  # list of 2D pixel measurements of the lander origin\n",
    "CAM_NAME = \"Left\"\n",
    "model = LangSAM()\n",
    "SEG_SCORE_THRESH = 0.4  # requires more testing\n",
    "img_folder = os.path.join(data_path, CAM_NAME)\n",
    "\n",
    "lander_orig_pix_meas = []\n",
    "meas_model = MeasModelLanderSeg(\n",
    "    model, lander_pose_world, cam_pose, seg_thresh=SEG_SCORE_THRESH, K=K\n",
    ")\n",
    "\n",
    "for frame in frame_data[939:942]:\n",
    "    frame_id = frame[\"frame\"]\n",
    "    print(f\"Processing frame {frame_id}\")\n",
    "    img_name = os.path.join(img_folder, f\"{frame_id}.png\")\n",
    "\n",
    "    if os.path.exists(img_name):\n",
    "        lander_orig_pix_meas.append(\n",
    "            meas_model.get_lander_origin_pix_from_image(img_name, display=True)\n",
    "        )\n",
    "        print(\"True Pixels: \", lander_orig_pix_meas[-1])\n",
    "\n",
    "        # test reproj\n",
    "        rover_state = np.array(frame[\"pose\"])\n",
    "        rover_rpy, rover_tvec = get_roteuler_tvec_from_poselist(rover_state)\n",
    "        rover_state = np.concatenate([rover_tvec, np.zeros(3), rover_rpy])\n",
    "        print(\n",
    "            \"Reproj Pixels: \", meas_model.get_lander_origin_pix_from_proj(rover_state, display=True)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EKF\n",
    "v0 = np.zeros(3)\n",
    "x0 = np.hstack((gt_rover_tvecs[0][:3], v0, gt_rover_roteulers[0])).T\n",
    "\n",
    "init_r = 0.001\n",
    "init_v = 0.01\n",
    "init_angle = 0.001\n",
    "P0 = np.diag(\n",
    "    np.hstack(\n",
    "        (\n",
    "            np.ones(3) * init_r * init_r,\n",
    "            np.ones(3) * init_v * init_v,\n",
    "            np.ones(3) * init_angle * init_angle,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Process noise\n",
    "Q_r = 0.00**2\n",
    "Q_v = 0.00**2\n",
    "Q_angle = 0.00005**2\n",
    "Q = np.diag(np.hstack((np.ones(3) * Q_r, np.ones(3) * Q_v, np.ones(3) * Q_angle)))\n",
    "\n",
    "# Measurement noise\n",
    "R = np.diag(np.array([1, 1]))  # 2D pixel measurements\n",
    "\n",
    "x_store = np.zeros((len(gt_rover_tvecs), 9))\n",
    "P_store = np.zeros((len(gt_rover_tvecs), 9, 9))\n",
    "\n",
    "ekf = EKF(x0, P0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using fiducials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.expanduser(\"~/LunarAutonomyChallenge/output/lander_closeups/\")\n",
    "# ----- lander closeups -----------------------------------------------\n",
    "# true examples\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/31.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/65.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/97.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/147.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/181.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/265.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/353.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/369.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/411.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/457.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/493.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/545.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/1199.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/1269.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/1665.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/1705.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/1751.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/2043.png\n",
    "\n",
    "# difficult examples\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/799.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/867.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/1111.png\n",
    "\n",
    "# false examples\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/617.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/709.png\n",
    "# /home/lac/LunarAutonomyChallenge/output/lander_closeups/FrontLeft/763.png\n",
    "\n",
    "i = [\n",
    "    31,\n",
    "    65,\n",
    "    97,\n",
    "    147,\n",
    "    181,\n",
    "    265,\n",
    "    353,\n",
    "    369,\n",
    "    411,\n",
    "    457,\n",
    "    493,\n",
    "    545,\n",
    "    1199,\n",
    "    1269,\n",
    "    1665,\n",
    "    1705,\n",
    "    1751,\n",
    "    2043,\n",
    "    799,\n",
    "    867,\n",
    "    1111,\n",
    "    617,\n",
    "    709,\n",
    "    763,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment fiducials\n",
    "model = LangSAM()\n",
    "text_prompt = \"square.\"\n",
    "SEG_SCORE_THRESH = 0.4  # requires more testing\n",
    "\n",
    "# for ind in i:\n",
    "for ind in i:\n",
    "    I1 = cv.imread(os.path.join(data_path, \"FrontLeft\", f\"{ind}.png\"), cv.IMREAD_GRAYSCALE)\n",
    "    plt.imshow(I1, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "    image_seg_in = Image.open(os.path.join(data_path, \"FrontLeft\", f\"{ind}.png\")).convert(\"RGB\")\n",
    "    results = model.predict([image_seg_in], [text_prompt])\n",
    "    full_mask = np.zeros_like(image_seg_in).copy()\n",
    "    for i, mask in enumerate(results[0][\"masks\"]):\n",
    "        print(results[0][\"scores\"][i])\n",
    "        if results[0][\"scores\"][i] < SEG_SCORE_THRESH:\n",
    "            continue\n",
    "        full_mask[mask.astype(bool)] = 255\n",
    "\n",
    "    plt.imshow(full_mask, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ground truth trajectory data loading\n",
    "# traj_data = json.load(open(f\"{data_path}/data_log.json\"))\n",
    "# initial_pose = np.array(traj_data[\"initial_pose\"])\n",
    "# data_log_path = os.path.join(data_path, \"data_log.json\")\n",
    "# data_log = json.load(open(data_log_path))\n",
    "\n",
    "# # Lander known pose w.r.t. world frame\n",
    "# lander_pose_world = np.array(data_log[\"lander_pose_world\"])\n",
    "\n",
    "# # Rover trajectory w.r.t. world frame\n",
    "# rover_traj_keys = [\"timestamp\", \"pose\"]\n",
    "# frame_data = data_log[\"frames\"]\n",
    "# rover_traj_data = {key: [] for key in rover_traj_keys}\n",
    "# for frame in frame_data:\n",
    "#     rover_traj_data[\"timestamp\"].append(frame[\"timestamp\"])\n",
    "#     rover_traj_data[\"pose\"].append(frame[\"pose\"])\n",
    "# rover_traj_data[\"pose\"] = np.array(rover_traj_data[\"pose\"])\n",
    "\n",
    "# # Camera (front left) known pose w.r.t. rover body-fixed frame\n",
    "# CAM_KEY = \"front left\"\n",
    "# CAM_POS_KEYS = [\"x\", \"y\", \"z\"]\n",
    "# CAM_ANG_KEYS = [\"roll\", \"pitch\", \"yaw\"]\n",
    "# camera_geom = json.load(open(os.path.expanduser(\"~/LunarAutonomyChallenge/docs/geometry.json\")))[\"rover\"][\"cameras\"]\n",
    "# cam_front_left = camera_geom[CAM_KEY]\n",
    "# cam_front_left_pos = np.array([cam_front_left[key] for key in CAM_POS_KEYS])\n",
    "# rover_r_camfl = R.from_euler(\"xyz\", [cam_front_left[key] for key in CAM_ANG_KEYS], degrees=False).as_matrix()\n",
    "\n",
    "# # known fiducial corner positions\n",
    "# fiducial_group_centers = json.load(open(os.path.expanduser(\"~/LunarAutonomyChallenge/docs/geometry.json\")))[\"lander\"][\"fiducials\"]\n",
    "# centers_group_a = fiducial_group_centers[\"a\"]\n",
    "# centers_group_b = fiducial_group_centers[\"b\"]\n",
    "# centers_group_c = fiducial_group_centers[\"c\"]\n",
    "# centers_group_d = fiducial_group_centers[\"d\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use frame 35 as the initial frame -- first-ish frame with apriltags visible\n",
    "# init_frame = 35\n",
    "# # rover_init_pose  = rover_traj_data[\"pose\"][0, :, :]\n",
    "# rover_init_pose = rover_traj_data[\"pose\"][init_frame, :, :]\n",
    "# I1 = cv.imread(os.path.join(data_path, \"FrontLeft\", f\"{init_frame}.png\"), cv.IMREAD_GRAYSCALE)\n",
    "# plt.imshow(I1, cmap=\"gray\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract the fiducial shapes from the initial frame\n",
    "# model = LangSAM()\n",
    "# text_prompt = \"square.\"\n",
    "# image_seg_in = Image.open(os.path.join(data_path, \"FrontLeft\", f\"{init_frame}.png\")).convert(\"RGB\")\n",
    "# results = model.predict([image_seg_in], [text_prompt])\n",
    "# full_mask = np.zeros_like(image_seg_in).copy()\n",
    "# for i, mask in enumerate(results[0][\"masks\"]):\n",
    "#     print(results[0][\"scores\"][i])\n",
    "#     if results[0][\"scores\"][i] < 0.45:\n",
    "#         # TODO: the score threshold will require tuning\n",
    "#         continue\n",
    "#     full_mask[mask.astype(bool)] = 255\n",
    "\n",
    "# plt.imshow(full_mask, cmap=\"gray\")\n",
    "# plt.show()\n",
    "\n",
    "# # extract the center of each mask\n",
    "# mask_centers = []\n",
    "# for i, mask in enumerate(results[0][\"masks\"]):\n",
    "#     if results[0][\"scores\"][i] < 0.45:\n",
    "#         continue\n",
    "#     mask = mask.astype(bool)\n",
    "#     mask_center = np.mean(np.argwhere(mask), axis=0)\n",
    "#     mask_centers.append(mask_center)\n",
    "\n",
    "# # plot on image\n",
    "# plt.imshow(I1, cmap=\"gray\")\n",
    "# for center in mask_centers:\n",
    "#     plt.plot(center[1], center[0], \"ro\", markersize=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sort mask centers by y-coordinate\n",
    "# mask_centers = np.array(mask_centers)\n",
    "# mask_centers = mask_centers[np.argsort(mask_centers[:, 0])]\n",
    "\n",
    "# # sort into top-left top-right bottom-right bottom-left\n",
    "# top_centers = mask_centers[:2]\n",
    "# bottom_centers = mask_centers[2:]\n",
    "# top_centers = top_centers[np.argsort(top_centers[:, 1])]\n",
    "# bottom_centers = bottom_centers[np.argsort(bottom_centers[:, 1])]\n",
    "# bottom_centers = bottom_centers[::-1]\n",
    "# mask_centers = np.vstack([top_centers, bottom_centers])\n",
    "\n",
    "# # reverse the column order to match the fiducial group centers\n",
    "# mask_centers = mask_centers[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # solve PnP\n",
    "# # TODO: get which fidicucial group is in the photo via rover pose estimate\n",
    "# TAG_KEYS = [\"top left\", \"top right\", \"lower right\", \"lower left\"]\n",
    "# TAG_CENTER_KEYS = [\"x\", \"y\", \"z\"]\n",
    "# world_pts = np.array([[centers_group_d[key_name][key] for key in TAG_CENTER_KEYS] for key_name in TAG_KEYS])\n",
    "# image_pts = mask_centers\n",
    "\n",
    "# print(\"World points:\\n\", world_pts)\n",
    "# print(\"Image points:\\n\", image_pts)\n",
    "\n",
    "# pnp_success, camfl_rvec_world, camfl_tvec_world = cv.solvePnP(world_pts, mask_centers, K, None)\n",
    "# if pnp_success:\n",
    "#     # extract rotation and translation\n",
    "#     camfl_r_world, _ = cv.Rodrigues(camfl_rvec_world)\n",
    "#     print(\"Rotation matrix:\\n\", camfl_r_world)\n",
    "#     print(\"Translation vector:\\n\", camfl_tvec_world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from scipy.spatial.transform import Rotation as R\n",
    "# rover_init_pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AprilTag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.expanduser(\"~/LunarAutonomyChallenge/output/lander/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
