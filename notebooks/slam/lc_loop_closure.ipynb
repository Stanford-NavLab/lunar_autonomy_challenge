{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loosely-coupled loop closure\n",
    "\n",
    "Attempted to use GTSAM EssentialMatrixFactor with side camera pairs, but the EssentialMatrixFactor\n",
    "uses and essential matrix key and it is unclear how it interacts with pose keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from gtsam.symbol_shorthand import X\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from lightglue import viz2d\n",
    "\n",
    "from lac.slam.feature_tracker import FeatureTracker\n",
    "from lac.slam.slam import PoseGraph\n",
    "from lac.slam.visual_odometry import StereoVisualOdometry\n",
    "from lac.utils.plotting import plot_poses, plot_lander_3d\n",
    "from lac.utils.visualization import image_grid\n",
    "from lac.util import load_data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../../output/DataCollectionAgent/double_loop_preset1\"\n",
    "initial_pose, lander_pose, poses, imu_data, cam_config = load_data(data_path)\n",
    "\n",
    "fig = plot_poses(poses, no_axes=True, color=\"black\", name=\"Ground truth\")\n",
    "fig.update_layout(height=900, width=1600, scene_aspectmode=\"data\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose graph\n",
    "\n",
    "- Run VO and add odometry factors to graph. Designate every N-th pose as a keyframe (for loop closure checking)\n",
    "- For each new pose, check its distance to all other keyframes excluding most recent ones. If that distance is less\n",
    "  than a threshold, check the angle between the two poses. If that angle is less than a threshold, attempt to estimate\n",
    "  a relative pose for loop closure. If LightGlue finds sufficient matches and PnP is successful, add a loop closure factor.\n",
    "- If the loop closure factor is added, run optimization on the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = PoseGraph()\n",
    "graph.add_pose(0, initial_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run VO to get odometry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data logs\n",
    "data_path = \"../../../output/DataCollectionAgent/double_loop_preset1\"\n",
    "initial_pose, lander_pose, poses, imu_data, cam_config = load_data(data_path)\n",
    "left_path = Path(data_path) / \"FrontLeft\"\n",
    "right_path = Path(data_path) / \"FrontRight\"\n",
    "side_path = Path(data_path) / \"Right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo = StereoVisualOdometry(cam_config)\n",
    "svo_poses = []\n",
    "pose_deltas = []\n",
    "\n",
    "START_FRAME = 80\n",
    "END_FRAME = len(poses)\n",
    "\n",
    "print(\"Running VO...\")\n",
    "progress_bar = tqdm(range(START_FRAME, END_FRAME, 2), dynamic_ncols=True)\n",
    "\n",
    "for frame in progress_bar:\n",
    "    progress_bar.set_description(f\"Processing Frame: {frame}\")\n",
    "\n",
    "    img_name = f\"{frame:06}.png\"\n",
    "    left_img = cv2.imread(str(left_path / img_name), cv2.IMREAD_GRAYSCALE)\n",
    "    right_img = cv2.imread(str(right_path / img_name), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if frame == START_FRAME:\n",
    "        svo.initialize(poses[frame], left_img, right_img)\n",
    "        svo_poses.append(poses[frame])\n",
    "        continue\n",
    "\n",
    "    svo.track(left_img, right_img)\n",
    "    svo_poses.append(svo.rover_pose)\n",
    "    pose_deltas.append(svo.pose_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_to_index = {frame: i for i, frame in enumerate(range(START_FRAME, END_FRAME, 2))}\n",
    "index_to_frame = {i: frame for i, frame in enumerate(range(START_FRAME, END_FRAME, 2))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig = plot_poses(poses[START_FRAME:END_FRAME], fig=fig, no_axes=True, color=\"black\", name=\"Ground truth\")\n",
    "fig = plot_poses(svo_poses, fig=fig, no_axes=True, color=\"orange\", name=\"VO\")\n",
    "fig = plot_lander_3d(fig=fig, lander_height=lander_pose[2, 3], color=\"silver\")\n",
    "fig.update_layout(height=900, width=1600, scene_aspectmode=\"data\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect loop closures\n",
    "\n",
    "Position based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = FeatureTracker(cam_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = 987\n",
    "i2 = 2297\n",
    "frame1 = index_to_frame[i1]\n",
    "frame2 = index_to_frame[i2]\n",
    "\n",
    "side_img1 = cv2.imread(str(side_path / f\"{frame1:06}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "side_img2 = cv2.imread(str(side_path / f\"{frame2:06}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Feature matching\n",
    "feats1 = tracker.extract_feats(side_img1)\n",
    "feats2 = tracker.extract_feats(side_img2)\n",
    "matches = tracker.match_feats(feats1, feats2)\n",
    "\n",
    "points1 = feats1[\"keypoints\"][0][matches[:, 0]].cpu().numpy()\n",
    "points2 = feats2[\"keypoints\"][0][matches[:, 1]].cpu().numpy()\n",
    "\n",
    "viz2d.plot_images([side_img1, side_img2])\n",
    "viz2d.plot_matches(points1, points2, lw=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [284, 300, 615]\n",
    "indices += [1604, 1580, 2148]\n",
    "images = []\n",
    "\n",
    "for i in indices:\n",
    "    images.append(cv2.imread(str(side_path / f\"{index_to_frame[i]:06}.png\"), cv2.IMREAD_GRAYSCALE))\n",
    "\n",
    "image_grid(images, rows=2, cols=len(images) // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get a list of LC pairs\n",
    "loop_closures = [\n",
    "    (284, 1604),\n",
    "    (300, 1580),\n",
    "    (615, 2148),\n",
    "    (1025, 2648),\n",
    "    (1319, 3200),\n",
    "    (1319, 3300),\n",
    "    (1565, 3643),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_poses(poses[START_FRAME:END_FRAME], no_axes=True, color=\"black\", name=\"Ground truth\")\n",
    "fig = plot_poses(svo_poses, fig=fig, no_axes=True, color=\"orange\", name=\"VO\")\n",
    "for i, j in loop_closures:\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=[svo_poses[i][0, 3], svo_poses[j][0, 3]],\n",
    "            y=[svo_poses[i][1, 3], svo_poses[j][1, 3]],\n",
    "            z=[svo_poses[i][2, 3], svo_poses[j][2, 3]],\n",
    "            mode=\"markers+lines\",\n",
    "            marker=dict(color=\"red\", size=5),\n",
    "            line=dict(color=\"red\", width=5),\n",
    "            name=f\"LC {i}-{j}\",\n",
    "        )\n",
    "    )\n",
    "fig = plot_lander_3d(fig=fig, lander_height=lander_pose[2, 3], color=\"silver\")\n",
    "fig.update_layout(height=900, width=1600, scene_aspectmode=\"data\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"../../../results/slam/manual_loop_closures.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add loop closures\n",
    "\n",
    "With gtsam EssentialMatrixFactor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtsam\n",
    "from gtsam.symbol_shorthand import X\n",
    "\n",
    "from lac.slam.loop_closure import estimate_loop_closure_pose\n",
    "from lac.slam.slam import K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = gtsam.NonlinearFactorGraph()\n",
    "values = gtsam.Values()\n",
    "\n",
    "sigma_t = 0.005  # [m]\n",
    "sigma_R = 0.00087  # [rad]\n",
    "svo_pose_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([sigma_R, sigma_R, sigma_R, sigma_t, sigma_t, sigma_t]))\n",
    "\n",
    "values.insert(X(0), gtsam.Pose3(initial_pose))\n",
    "graph.add(gtsam.NonlinearEqualityPose3(X(0), gtsam.Pose3(initial_pose)))\n",
    "\n",
    "i = 1\n",
    "for frame in tqdm(np.arange(START_FRAME + 2, END_FRAME, 2)):\n",
    "    values.insert(X(i), gtsam.Pose3(svo_poses[i]))\n",
    "    graph.push_back(gtsam.BetweenFactorPose3(X(i - 1), X(i), gtsam.Pose3(pose_deltas[i - 1]), svo_pose_noise))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essential matrix factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_model = gtsam.noiseModel.Isotropic.Sigma(2, 3.0)\n",
    "\n",
    "# for i, j in loop_closures:\n",
    "#     img_i = cv2.imread(str(side_path / f\"{index_to_frame[i]:06}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "#     img_j = cv2.imread(str(side_path / f\"{index_to_frame[j]:06}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "#     feats_i = tracker.extract_feats(img_i)\n",
    "#     feats_j = tracker.extract_feats(img_j)\n",
    "#     matches = tracker.match_feats(feats_i, feats_j)\n",
    "#     matched_kps_i = feats_i[\"keypoints\"][0][matches[:, 0]].cpu().numpy()\n",
    "#     matched_kps_j = feats_j[\"keypoints\"][0][matches[:, 1]].cpu().numpy()\n",
    "\n",
    "#     graph.add(gtsam.EssentialMatrixFactor(X(i), X(j), matched_kps_i, matched_kps_j, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PnP relative pose estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_model = gtsam.noiseModel.Isotropic.Sigma(2, 3.0)\n",
    "\n",
    "for i, j in loop_closures:\n",
    "    img_i = cv2.imread(str(side_path / f\"{index_to_frame[i]:06}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "    img_j = cv2.imread(str(side_path / f\"{index_to_frame[j]:06}.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "    feats_i = tracker.extract_feats(img_i)\n",
    "    feats_j = tracker.extract_feats(img_j)\n",
    "    matches = tracker.match_feats(feats_i, feats_j)\n",
    "    matched_kps_i = feats_i[\"keypoints\"][0][matches[:, 0]].cpu().numpy()\n",
    "    matched_kps_j = feats_j[\"keypoints\"][0][matches[:, 1]].cpu().numpy()\n",
    "\n",
    "    graph.add(gtsam.EssentialMatrixFactor(X(i), X(j), matched_kps_i, matched_kps_j, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
