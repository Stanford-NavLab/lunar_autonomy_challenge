{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from gtsam.symbol_shorthand import X\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lac.slam.feature_tracker import FeatureTracker\n",
    "from lac.utils.plotting import plot_poses, plot_surface, plot_3d_points\n",
    "from lac.util import load_data, load_stereo_images\n",
    "from lac.params import LAC_BASE_PATH\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data logs\n",
    "data_path = Path(LAC_BASE_PATH) / \"output/DataCollectionAgent/stereo_lights1.0_map1_preset1\"\n",
    "initial_pose, lander_pose, poses, imu_data, cam_config = load_data(data_path)\n",
    "print(f\"Loaded {len(poses)} poses\")\n",
    "\n",
    "# Load the images\n",
    "left_imgs, right_imgs = load_stereo_images(data_path)\n",
    "\n",
    "# Load the ground truth map\n",
    "map = np.load(\n",
    "    Path(LAC_BASE_PATH) / \"data/heightmaps/competition/Moon_Map_01_preset_0.dat\",\n",
    "    allow_pickle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_poses(poses, no_axes=True, color=\"black\", name=\"Ground Truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = FeatureTracker(cam_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 2 images from similar viewpoints\n",
    "frame1 = 600\n",
    "frame2 = 4300\n",
    "img1 = left_imgs[frame1]\n",
    "img2 = left_imgs[frame2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax[0].imshow(img1, cmap=\"gray\")\n",
    "ax[1].imshow(img2, cmap=\"gray\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats1 = tracker.extract_feats(img1)\n",
    "feats2 = tracker.extract_feats(img2)\n",
    "matches = tracker.match_feats(feats1, feats2)\n",
    "\n",
    "points1 = feats1[\"keypoints\"][0][matches[:, 0]].cpu().numpy()\n",
    "points2 = feats2[\"keypoints\"][0][matches[:, 1]].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightglue import viz2d\n",
    "\n",
    "viz2d.plot_images([img1, img2])\n",
    "viz2d.plot_matches(points1, points2, lw=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop closure factors\n",
    "\n",
    "Assuming the loop closure has been detected:\n",
    "\n",
    "1. Stereo matching: For each frame, triangulate stereo points. Then, match triangulated points in frame 1 with triangulated points in frame 2.\n",
    "2. We should then be able to use these matches and corresponding depth estimates to estimate a relative pose estimate between the two frames, and add this to the graph\n",
    "\n",
    "We should also use the matches (doesn't have to be stereo anymore) to associate landmarks IDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate relative pose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from lac.slam.feature_tracker import prune_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 3D-3D correspondences, and ICP (not working)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_left1, feats_right1, stereo_matches1, depths1 = tracker.process_stereo(\n",
    "    left_imgs[frame1], right_imgs[frame1]\n",
    ")\n",
    "feats_left2, feats_right2, stereo_matches2, depths2 = tracker.process_stereo(\n",
    "    left_imgs[frame2], right_imgs[frame2]\n",
    ")\n",
    "\n",
    "feats_left1_triangulated = prune_features(feats_left1, stereo_matches1[:, 0])\n",
    "feats_left2_triangulated = prune_features(feats_left2, stereo_matches2[:, 1])\n",
    "print(f\"Triangulated {len(feats_left1_triangulated['keypoints'][0])} features in image 1\")\n",
    "print(f\"Triangulated {len(feats_left2_triangulated['keypoints'][0])} features in image 2\")\n",
    "\n",
    "triangulated_matches12 = tracker.match_feats(feats_left1_triangulated, feats_left2_triangulated)\n",
    "print(f\"Found {len(triangulated_matches12)} matches between the two sets of triangulated features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_feats1 = prune_features(feats_left1_triangulated, triangulated_matches12[:, 0])\n",
    "matched_pts_left1 = matched_feats1[\"keypoints\"][0]\n",
    "depths1_matched = depths1[triangulated_matches12[:, 0]]\n",
    "points_local1 = tracker.project_stereo(np.eye(4), matched_pts_left1, depths1_matched)\n",
    "\n",
    "matched_feats2 = prune_features(feats_left2_triangulated, triangulated_matches12[:, 1])\n",
    "matched_pts_left2 = matched_feats2[\"keypoints\"][0]\n",
    "depths2_matched = depths2[triangulated_matches12[:, 1]]\n",
    "points_local2 = tracker.project_stereo(np.eye(4), matched_pts_left2, depths2_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_3d_points(points_local1, color=\"red\", name=\"Points Local 1\")\n",
    "fig = plot_3d_points(points_local2, fig=fig, color=\"blue\", name=\"Points Local 2\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd1 = o3d.geometry.PointCloud()\n",
    "pcd2 = o3d.geometry.PointCloud()\n",
    "pcd1.points = o3d.utility.Vector3dVector(points_local1)\n",
    "pcd2.points = o3d.utility.Vector3dVector(points_local2)\n",
    "\n",
    "# Run ICP\n",
    "threshold = 0.1\n",
    "icp_result = o3d.pipelines.registration.registration_icp(\n",
    "    pcd1,\n",
    "    pcd2,\n",
    "    threshold,\n",
    "    np.eye(4),\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    ")\n",
    "\n",
    "T_icp = icp_result.transformation\n",
    "print(\"Estimated Rigid Transformation (ICP):\\n\", T_icp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, M, inliers = cv2.estimateAffine3D(points_local1, points_local2)\n",
    "print(f\"Number of inliers: {np.sum(inliers)} out of {len(inliers)}\")\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_pose1 = poses[frame1] @ np.linalg.inv(T_icp)\n",
    "fig = plot_poses([poses[frame1], poses[frame2], est_pose1])\n",
    "fig = plot_3d_points(\n",
    "    poses[frame1][:3, 3][None, :], fig=fig, color=\"blue\", markersize=10, name=\"Pose 1\"\n",
    ")\n",
    "fig = plot_3d_points(\n",
    "    poses[frame2][:3, 3][None, :], fig=fig, color=\"red\", markersize=10, name=\"Pose 2\"\n",
    ")\n",
    "fig = plot_3d_points(\n",
    "    est_pose1[:3, 3][None, :], fig=fig, color=\"green\", markersize=10, name=\"Estimated Pose 2\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 2D-3D correspondences, and PnP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.utils.frames import (\n",
    "    invert_transform_mat,\n",
    "    OPENCV_TO_CAMERA_PASSIVE,\n",
    ")\n",
    "from lac.params import CAMERA_INTRINSICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_left1, feats_right1, stereo_matches1, depths1 = tracker.process_stereo(\n",
    "    left_imgs[frame1], right_imgs[frame1]\n",
    ")\n",
    "feats_left2, feats_right2, stereo_matches2, depths2 = tracker.process_stereo(\n",
    "    left_imgs[frame2], right_imgs[frame2]\n",
    ")\n",
    "\n",
    "matched_feats1 = prune_features(feats_left1, stereo_matches1[:, 0])\n",
    "matched_pts_left1 = matched_feats1[\"keypoints\"][0]\n",
    "points_local1 = tracker.project_stereo(np.eye(4), matched_pts_left1, depths1)\n",
    "\n",
    "matches12_left = tracker.match_feats(feats_left1, feats_left2)\n",
    "\n",
    "stereo_indices = stereo_matches1[:, 0]\n",
    "frame_indices = matches12_left[:, 0]\n",
    "\n",
    "common_indices = torch.tensor(\n",
    "    list(set(stereo_indices.cpu().numpy()) & set(frame_indices.cpu().numpy()))\n",
    ").cuda()\n",
    "frame_common = matches12_left[torch.isin(frame_indices, common_indices)]\n",
    "\n",
    "points3D = points_local1[torch.isin(stereo_indices, common_indices).cpu().numpy()]\n",
    "points2D = feats_left2[\"keypoints\"][0][frame_common[:, 1]].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, rvec, tvec, inliers = cv2.solvePnPRansac(\n",
    "    objectPoints=points3D,\n",
    "    imagePoints=points2D,\n",
    "    cameraMatrix=CAMERA_INTRINSICS,\n",
    "    distCoeffs=None,\n",
    "    flags=cv2.SOLVEPNP_ITERATIVE,\n",
    "    reprojectionError=8.0,\n",
    "    iterationsCount=100,\n",
    ")\n",
    "\n",
    "R, _ = cv2.Rodrigues(rvec)\n",
    "T = np.hstack((R, tvec))\n",
    "est_pose = np.vstack((T, [0, 0, 0, 1]))\n",
    "w_T_c = invert_transform_mat(est_pose)\n",
    "w_T_c[:3, :3] = w_T_c[:3, :3] @ OPENCV_TO_CAMERA_PASSIVE\n",
    "rel_pose = w_T_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_pose2 = poses[frame1] @ rel_pose\n",
    "fig = plot_poses([poses[frame1], poses[frame2], est_pose2])\n",
    "fig = plot_3d_points(\n",
    "    poses[frame1][:3, 3][None, :], fig=fig, color=\"blue\", markersize=10, name=\"Pose 1\"\n",
    ")\n",
    "fig = plot_3d_points(\n",
    "    poses[frame2][:3, 3][None, :], fig=fig, color=\"red\", markersize=10, name=\"Pose 2\"\n",
    ")\n",
    "fig = plot_3d_points(\n",
    "    est_pose2[:3, 3][None, :], fig=fig, color=\"green\", markersize=10, name=\"Estimated Pose 2\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"pnp_loop_closure_poses.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.slam.visual_odometry import StereoVisualOdometry\n",
    "from lac.slam.loop_closure import estimate_loop_closure_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo = StereoVisualOdometry(cam_config)\n",
    "START_FRAME = 80\n",
    "svo.initialize(initial_pose, left_imgs[START_FRAME], right_imgs[START_FRAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtsam\n",
    "from gtsam.symbol_shorthand import B, V, X\n",
    "\n",
    "graph = gtsam.NonlinearFactorGraph()\n",
    "values = gtsam.Values()\n",
    "\n",
    "svo_pose_sigma = 1e-2 * np.ones(6)\n",
    "svo_pose_noise = gtsam.noiseModel.Diagonal.Sigmas(svo_pose_sigma)\n",
    "\n",
    "# # Translation sigma (meters)\n",
    "# sigma_t = 0.005  # 5 mm\n",
    "\n",
    "# # Rotation sigma (radians)\n",
    "# sigma_R = 0.00087  # ~0.05 degrees\n",
    "\n",
    "# # Covariance matrix (6x6 diagonal)\n",
    "# pose_noise = gtsam.noiseModel.Diagonal.Sigmas(\n",
    "#     np.array([sigma_R, sigma_R, sigma_R, sigma_t, sigma_t, sigma_t])\n",
    "# )\n",
    "\n",
    "values.insert(X(0), gtsam.Pose3(initial_pose))\n",
    "graph.add(gtsam.NonlinearEqualityPose3(X(0), gtsam.Pose3(initial_pose)))\n",
    "\n",
    "svo_poses = [initial_pose]\n",
    "pose_deltas = []\n",
    "\n",
    "END_FRAME = 4500\n",
    "i = 1\n",
    "\n",
    "for frame in tqdm(np.arange(START_FRAME + 2, END_FRAME, 2)):\n",
    "    svo.track(left_imgs[frame], right_imgs[frame])\n",
    "    svo_poses.append(svo.rover_pose)\n",
    "    values.insert(X(i), gtsam.Pose3(svo_poses[i]))\n",
    "    graph.push_back(\n",
    "        gtsam.BetweenFactorPose3(X(i - 1), X(i), gtsam.Pose3(svo.pose_delta), svo_pose_noise)\n",
    "    )\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame1 = 600\n",
    "frame2 = 4300\n",
    "i1 = int((frame1 - START_FRAME) / 2)\n",
    "i2 = int((frame2 - START_FRAME) / 2)\n",
    "\n",
    "rel_pose = estimate_loop_closure_pose(\n",
    "    tracker, left_imgs[frame1], right_imgs[frame1], left_imgs[frame2], right_imgs[frame2]\n",
    ")\n",
    "\n",
    "lc_pose_sigma = 1e-1 * np.ones(6)\n",
    "lc_pose_noise = gtsam.noiseModel.Diagonal.Sigmas(svo_pose_sigma)\n",
    "graph.push_back(gtsam.BetweenFactorPose3(X(i1), X(i2), gtsam.Pose3(rel_pose), lc_pose_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = gtsam.LevenbergMarquardtParams()\n",
    "optimizer = gtsam.LevenbergMarquardtOptimizer(graph, values, params)\n",
    "result = optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_poses = []\n",
    "for i in range(len(svo_poses)):\n",
    "    opt_poses.append(result.atPose3(X(i)).matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_poses(poses[:END_FRAME], no_axes=True, color=\"black\", name=\"Ground Truth\")\n",
    "fig = plot_poses(svo_poses, no_axes=True, fig=fig, color=\"red\", name=\"VO\")\n",
    "fig = plot_poses(opt_poses, no_axes=True, fig=fig, color=\"green\", name=\"Optimized\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop closure detection\n",
    "\n",
    "- Position based: assuming drift is not too high, attempt to match frames which have similar pose estimates.\n",
    "- Visual based: compute bag-of-words (BoW) descriptors for each frame and form a visual vocabulary. Then, match frames based on the BoW descriptors.\n",
    "\n",
    "We can also program in some purposeful loop closure maneuvers, like turn and look at the lander\n",
    "every N frames.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
