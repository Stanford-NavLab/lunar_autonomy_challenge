{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from gtsam.symbol_shorthand import X\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import inv\n",
    "\n",
    "from lac.utils.frames import get_cam_pose_rover, CAMERA_TO_OPENCV_PASSIVE\n",
    "from lac.perception.vision import get_camera_intrinsics\n",
    "from lac.slam.slam import SLAM\n",
    "from lac.slam.visual_odometry import StereoVisualOdometry\n",
    "from lac.slam.feature_tracker import FeatureTracker\n",
    "from lac.utils.plotting import plot_poses, plot_surface, plot_3d_points\n",
    "from lac.util import load_data, load_stereo_images, load_side_images\n",
    "from lac.params import CAMERA_INTRINSICS\n",
    "from lac.utils.frames import (\n",
    "    apply_transform,\n",
    "    invert_transform_mat,\n",
    "    OPENCV_TO_CAMERA_PASSIVE,\n",
    "    get_cam_pose_rover,\n",
    ")\n",
    "from lac.params import LAC_BASE_PATH\n",
    "\n",
    "print(LAC_BASE_PATH)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data logs\n",
    "data_path = \"/home/shared/data_raw/LAC/segmentation/slam_map1_preset1_teleop\"\n",
    "initial_pose, lander_pose, poses, imu_data, cam_config = load_data(data_path)\n",
    "print(f\"Loaded {len(poses)} poses\")\n",
    "# Load the images\n",
    "left_imgs, right_imgs = load_stereo_images(data_path)\n",
    "side_left_imgs, side_right_imgs = load_side_images(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ground truth map\n",
    "map = np.load(\n",
    "    \"/home/shared/data_raw/LAC/heightmaps/competition/Moon_Map_01_preset_0.dat\",\n",
    "    allow_pickle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_FRAME = 100\n",
    "\n",
    "IMG_RATE = 2\n",
    "KEYFRAME_RATE = 10\n",
    "GRAPH_UPDATE_RATE = 2\n",
    "GRAPH_OPTIMIZE_RATE = 1000\n",
    "END_FRAME = 5654\n",
    "\n",
    "curr_pose = initial_pose\n",
    "step = START_FRAME\n",
    "prev_step = None\n",
    "prev_pose = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track\n",
    "svo = StereoVisualOdometry(cam_config)\n",
    "svo.initialize(initial_pose, left_imgs[START_FRAME], right_imgs[START_FRAME])\n",
    "\n",
    "prev_step = step\n",
    "step = START_FRAME + IMG_RATE\n",
    "\n",
    "svo.track(left_imgs[step], right_imgs[step])\n",
    "\n",
    "prev_pose = curr_pose\n",
    "curr_pose = svo.get_pose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = FeatureTracker(cam_config)\n",
    "\n",
    "cam_name = \"Left\"\n",
    "\n",
    "if cam_name == \"Right\":\n",
    "    prev_img = side_right_imgs[prev_step]\n",
    "    next_img = side_right_imgs[step]\n",
    "else:\n",
    "    prev_img = side_left_imgs[prev_step]\n",
    "    next_img = side_left_imgs[step]\n",
    "\n",
    "prev_feats = tracker.extract_feats(prev_img)\n",
    "next_feats = tracker.extract_feats(next_img)\n",
    "\n",
    "matches = tracker.match_feats(prev_feats, next_feats, min_score=0.9)\n",
    "points_prev = prev_feats[\"keypoints\"][0][matches[:, 0]]\n",
    "points_next = next_feats[\"keypoints\"][0][matches[:, 1]]\n",
    "\n",
    "points2d_prev = points_prev.cpu().numpy()\n",
    "points2d_next = points_next.cpu().numpy()\n",
    "\n",
    "# Camera intrinsics and extrinsics\n",
    "K = get_camera_intrinsics(cam_name, cam_config)\n",
    "rover_T_cam = get_cam_pose_rover(cam_name)\n",
    "rover_T_cam_ocv = rover_T_cam.copy()\n",
    "rover_T_cam_ocv[:3, :3] = rover_T_cam_ocv[:3, :3] @ CAMERA_TO_OPENCV_PASSIVE\n",
    "\n",
    "# Projection matrices\n",
    "rover_poses = [curr_pose]\n",
    "for _ in range(10):\n",
    "    cam_T_world_0 = inv(prev_pose @ rover_T_cam_ocv)\n",
    "    cam_T_world_1 = inv(rover_poses[-1] @ rover_T_cam_ocv)\n",
    "\n",
    "    P0 = K @ cam_T_world_0[:3]\n",
    "    P1 = K @ cam_T_world_1[:3]\n",
    "\n",
    "    MAX_DEPTH = 10.0\n",
    "\n",
    "    points_4d_h = cv2.triangulatePoints(P1, P0, points2d_next.T, points2d_prev.T)\n",
    "    points_3d_next = (points_4d_h[:3] / points_4d_h[3]).T\n",
    "    depths_next = (cam_T_world_1[:3, :3] @ points_3d_next.T + cam_T_world_1[:3, 3:4]).T[:, 2]\n",
    "    depth_mask = (depths_next > 0) & (depths_next < MAX_DEPTH)\n",
    "\n",
    "    # Initial guess\n",
    "    rover_to_cam = get_cam_pose_rover(cam_name)\n",
    "    cam_to_rover = invert_transform_mat(rover_to_cam)\n",
    "    w_T_c = rover_pose @ rover_to_cam\n",
    "    w_T_c[:3, :3] = w_T_c[:3, :3] @ np.linalg.inv(OPENCV_TO_CAMERA_PASSIVE)\n",
    "    est_pose = invert_transform_mat(w_T_c)\n",
    "    R = est_pose[:3, :3]\n",
    "    tvec2 = est_pose[:3, 3].reshape(3, 1)\n",
    "    rvec2, _ = cv2.Rodrigues(R)\n",
    "\n",
    "    success, rvec, tvec, inliers = cv2.solvePnPRansac(\n",
    "        objectPoints=points_3d_next[depth_mask],\n",
    "        imagePoints=points2d_next[depth_mask],\n",
    "        cameraMatrix=CAMERA_INTRINSICS,\n",
    "        distCoeffs=None,\n",
    "        flags=cv2.SOLVEPNP_ITERATIVE,\n",
    "        reprojectionError=0.1,\n",
    "        iterationsCount=100,\n",
    "        useExtrinsicGuess=True,\n",
    "        rvec=rvec2,\n",
    "        tvec=tvec2,\n",
    "    )\n",
    "\n",
    "    if success:\n",
    "        # TODO: clean up this code\n",
    "        R, _ = cv2.Rodrigues(rvec)\n",
    "        T = np.hstack((R, tvec))\n",
    "        est_pose = np.vstack((T, [0, 0, 0, 1]))\n",
    "        w_T_c = invert_transform_mat(est_pose)\n",
    "        w_T_c[:3, :3] = w_T_c[:3, :3] @ OPENCV_TO_CAMERA_PASSIVE\n",
    "        rover_to_cam = get_cam_pose_rover(cam_name)\n",
    "        cam_to_rover = invert_transform_mat(rover_to_cam)\n",
    "        rover_pose = w_T_c @ cam_to_rover\n",
    "\n",
    "    rover_poses.append(rover_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_errors = []\n",
    "for i in range(len(rover_poses)):\n",
    "    t_errors.append(np.linalg.norm(rover_poses[i][:3, 3] - poses[step][:3, 3]))\n",
    "\n",
    "t_errors = np.array(t_errors)\n",
    "plt.plot(t_errors * 100)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Translation Error (cm)\")\n",
    "print(\"Initial error\", t_errors[0])\n",
    "print(\"Final error\", t_errors[-1])\n",
    "print(\"Improvement\", t_errors[0] - t_errors[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(points2d_next[:, 1], depths_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(next_img, cmap=\"gray\")\n",
    "for i in range(len(points2d_prev)):\n",
    "    if not depth_mask[i]:\n",
    "        continue\n",
    "    plt.plot([points2d_prev[i, 0], points2d_next[i, 0]], [points2d_prev[i, 1], points2d_next[i, 1]], color=\"lime\")\n",
    "    x, y = points2d_next[i]\n",
    "    plt.scatter(x, y, color=\"lime\", s=5)\n",
    "    plt.text(x + 2, y, f\"{depths_next[i]:.2f}\", color=\"red\", fontsize=8)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svo_poses = [initial_pose]\n",
    "curr_pose = initial_pose\n",
    "\n",
    "END_FRAME = 5654\n",
    "END_FRAME = 1000\n",
    "\n",
    "svo = StereoVisualOdometry(cam_config)\n",
    "svo.initialize(initial_pose, left_imgs[START_FRAME], right_imgs[START_FRAME])\n",
    "\n",
    "# Main loop over image frames\n",
    "for curr_step in tqdm(range(START_FRAME + IMG_RATE, END_FRAME + 1, IMG_RATE)):\n",
    "    if curr_step >= max(left_imgs.keys()):\n",
    "        print(\"Reached end of images\")\n",
    "        break\n",
    "    # Run VO for real-time pose tracking\n",
    "    svo.track(left_imgs[curr_step], right_imgs[curr_step])\n",
    "    curr_pose = svo.get_pose()\n",
    "    svo_poses.append(curr_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pose_mono(cam_name, prev_img, next_img, curr_pose, n_iter=20):\n",
    "\n",
    "    new_pose = curr_pose.copy()\n",
    "\n",
    "    prev_feats = tracker.extract_feats(prev_img)\n",
    "    next_feats = tracker.extract_feats(next_img)\n",
    "\n",
    "    matches = tracker.match_feats(prev_feats, next_feats, min_score=0.9)\n",
    "    points_prev = prev_feats[\"keypoints\"][0][matches[:, 0]]\n",
    "    points_next = next_feats[\"keypoints\"][0][matches[:, 1]]\n",
    "\n",
    "    points2d_prev = points_prev.cpu().numpy()\n",
    "    points2d_next = points_next.cpu().numpy()\n",
    "\n",
    "    # Camera intrinsics and extrinsics\n",
    "    K = get_camera_intrinsics(cam_name, cam_config)\n",
    "    rover_T_cam = get_cam_pose_rover(cam_name)\n",
    "    rover_T_cam_ocv = rover_T_cam.copy()\n",
    "    rover_T_cam_ocv[:3, :3] = rover_T_cam_ocv[:3, :3] @ CAMERA_TO_OPENCV_PASSIVE\n",
    "\n",
    "    # Projection matrices\n",
    "    for it in range(n_iter):\n",
    "        cam_T_world_0 = inv(prev_pose @ rover_T_cam_ocv)\n",
    "        cam_T_world_1 = inv(new_pose @ rover_T_cam_ocv)\n",
    "\n",
    "        P0 = K @ cam_T_world_0[:3]\n",
    "        P1 = K @ cam_T_world_1[:3]\n",
    "\n",
    "        MAX_DEPTH = 15.0\n",
    "\n",
    "        points_4d_h = cv2.triangulatePoints(P1, P0, points2d_next.T, points2d_prev.T)\n",
    "        points_3d_next = (points_4d_h[:3] / points_4d_h[3]).T\n",
    "        depths_next = (cam_T_world_1[:3, :3] @ points_3d_next.T + cam_T_world_1[:3, 3:4]).T[:, 2]\n",
    "        depth_mask = (depths_next > 0) & (depths_next < MAX_DEPTH)\n",
    "\n",
    "        if depth_mask.sum() < 20:\n",
    "            print(f\"Not enough points in depth mask {depth_mask.sum()}, iteration {it}\")\n",
    "            break\n",
    "\n",
    "        # Initial guess\n",
    "        rover_to_cam = get_cam_pose_rover(cam_name)\n",
    "        cam_to_rover = invert_transform_mat(rover_to_cam)\n",
    "        w_T_c = new_pose @ rover_to_cam\n",
    "        w_T_c[:3, :3] = w_T_c[:3, :3] @ np.linalg.inv(OPENCV_TO_CAMERA_PASSIVE)\n",
    "        est_pose = invert_transform_mat(w_T_c)\n",
    "        R = est_pose[:3, :3]\n",
    "        tvec2 = est_pose[:3, 3].reshape(3, 1)\n",
    "        rvec2, _ = cv2.Rodrigues(R)\n",
    "\n",
    "        success, rvec, tvec, inliers = cv2.solvePnPRansac(\n",
    "            objectPoints=points_3d_next[depth_mask],\n",
    "            imagePoints=points2d_next[depth_mask],\n",
    "            cameraMatrix=CAMERA_INTRINSICS,\n",
    "            distCoeffs=None,\n",
    "            flags=cv2.SOLVEPNP_ITERATIVE,\n",
    "            reprojectionError=0.1,\n",
    "            iterationsCount=100,\n",
    "            useExtrinsicGuess=True,\n",
    "            rvec=rvec2,\n",
    "            tvec=tvec2,\n",
    "        )\n",
    "\n",
    "        if success:\n",
    "            # TODO: clean up this code\n",
    "            R, _ = cv2.Rodrigues(rvec)\n",
    "            T = np.hstack((R, tvec))\n",
    "            est_pose = np.vstack((T, [0, 0, 0, 1]))\n",
    "            w_T_c = invert_transform_mat(est_pose)\n",
    "            w_T_c[:3, :3] = w_T_c[:3, :3] @ OPENCV_TO_CAMERA_PASSIVE\n",
    "            rover_to_cam = get_cam_pose_rover(cam_name)\n",
    "            cam_to_rover = invert_transform_mat(rover_to_cam)\n",
    "            new_pose = w_T_c @ cam_to_rover\n",
    "        else:\n",
    "            print(\"PnP failed\")\n",
    "            break\n",
    "\n",
    "    return new_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_svo_poses = [initial_pose]\n",
    "\n",
    "END_FRAME = 5654\n",
    "END_FRAME = 1000\n",
    "\n",
    "curr_pose = initial_pose\n",
    "curr_step = START_FRAME\n",
    "prev_step = curr_step\n",
    "prev_pose = curr_pose\n",
    "\n",
    "svo = StereoVisualOdometry(cam_config)\n",
    "svo.initialize(initial_pose, left_imgs[START_FRAME], right_imgs[START_FRAME])\n",
    "\n",
    "# Main loop over image frames\n",
    "for curr_step in tqdm(range(START_FRAME + IMG_RATE, END_FRAME + 1, IMG_RATE)):\n",
    "    if curr_step >= max(left_imgs.keys()):\n",
    "        print(\"Reached end of images\")\n",
    "        break\n",
    "\n",
    "    # Run VO for real-time pose tracking\n",
    "    svo.track(left_imgs[curr_step], right_imgs[curr_step])\n",
    "    curr_pose = svo.get_pose()\n",
    "\n",
    "    # Update\n",
    "    prev_img_right = side_right_imgs[prev_step]\n",
    "    next_img_right = side_right_imgs[curr_step]\n",
    "    curr_pose = update_pose_mono(\"Right\", prev_img_right, next_img_right, curr_pose, n_iter=20)\n",
    "\n",
    "    prev_img_left = side_left_imgs[prev_step]\n",
    "    next_img_left = side_left_imgs[curr_step]\n",
    "    curr_pose = update_pose_mono(\"Left\", prev_img_left, next_img_left, curr_pose, n_iter=20)\n",
    "\n",
    "    iter_svo_poses.append(curr_pose)\n",
    "\n",
    "    rmse = np.linalg.norm(curr_pose[:3, 3] - poses[curr_step][:3, 3])\n",
    "\n",
    "    prev_pose = curr_pose\n",
    "    prev_step = curr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_surface(map, showscale=False)\n",
    "fig = plot_poses(poses[:END_FRAME], fig=fig, no_axes=True, color=\"black\", name=\"Ground truth\")\n",
    "fig = plot_poses(svo_poses, fig=fig, no_axes=True, color=\"orange\", name=\"VO poses\")\n",
    "fig = plot_poses(iter_svo_poses, fig=fig, no_axes=True, color=\"red\", name=\"Iter VO poses\", markersize=2)\n",
    "# fig = plot_3d_points(landmark_points_cropped, fig=fig, color=\"red\", markersize=2, name=\"Landmarks\")\n",
    "fig.update_layout(height=900, width=1600, scene_aspectmode=\"data\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"gtsam_slam.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
