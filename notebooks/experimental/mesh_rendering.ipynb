{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch3d.org/tutorials/render_textured_meshes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
    "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights,\n",
    "    DirectionalLights,\n",
    "    Materials,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader,\n",
    "    TexturesUV,\n",
    "    TexturesVertex,\n",
    ")\n",
    "\n",
    "# add path for demo utils functions\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Render textured meshes\n",
    "\n",
    "https://pytorch3d.org/tutorials/render_textured_meshes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load mesh and texture file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/cow_mesh\n",
    "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.obj\n",
    "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.mtl\n",
    "!wget -P data/cow_mesh https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow_texture.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = \"./data\"\n",
    "obj_filename = os.path.join(DATA_DIR, \"cow_mesh/cow.obj\")\n",
    "\n",
    "# Load obj file\n",
    "mesh = load_objs_as_meshes([obj_filename], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "texture_image = mesh.textures.maps_padded()\n",
    "plt.imshow(texture_image.squeeze().cpu().numpy())\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "texturesuv_image_matplotlib(mesh.textures, subsample=None)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a renderer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a camera.\n",
    "# With world coordinates +Y up, +X left and +Z in, the front of the cow is facing the -Z direction.\n",
    "# So we move the camera by 180 in the azimuth direction so it is facing the front of the cow.\n",
    "R, T = look_at_view_transform(2.7, 0, 180)\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
    "# and blur_radius=0.0. We also set bin_size and max_faces_per_bin to None which ensure that\n",
    "# the faster coarse-to-fine rasterization method is used. Refer to rasterize_meshes.py for\n",
    "# explanations of these parameters. Refer to docs/notes/renderer.md for an explanation of\n",
    "# the difference between naive and coarse-to-fine rasterization.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=512,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    ")\n",
    "\n",
    "# Place a point light in front of the object. As mentioned above, the front of the cow is facing the\n",
    "# -z direction.\n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "# Create a Phong renderer by composing a rasterizer and a shader. The textured Phong shader will\n",
    "# interpolate the texture uv coordinates for each vertex, sample from a texture image and\n",
    "# apply the Phong lighting model\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(cameras=cameras, raster_settings=raster_settings),\n",
    "    shader=SoftPhongShader(device=device, cameras=cameras, lights=lights),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = renderer(mesh)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now move the light so it is on the +Z axis which will be behind the cow.\n",
    "lights.location = torch.tensor([0.0, 0.0, +1.0], device=device)[None]\n",
    "images = renderer(mesh, lights=lights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the object by increasing the elevation and azimuth angles\n",
    "R, T = look_at_view_transform(dist=2.7, elev=10, azim=-150)\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "# Move the light location so the light is shining on the cow's face.\n",
    "lights.location = torch.tensor([[2.0, 2.0, -2.0]], device=device)\n",
    "\n",
    "# Change specular color to green and change material shininess\n",
    "materials = Materials(device=device, specular_color=[[0.0, 1.0, 0.0]], shininess=10.0)\n",
    "\n",
    "# Re render the mesh, passing in keyword arguments for the modified components.\n",
    "images = renderer(mesh, lights=lights, materials=materials, cameras=cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(images[0, ..., :3].cpu().numpy())\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batched rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size - this is the number of different viewpoints from which we want to render the mesh.\n",
    "batch_size = 20\n",
    "\n",
    "# Create a batch of meshes by repeating the cow mesh and associated textures.\n",
    "# Meshes has a useful `extend` method which allows us do this very easily.\n",
    "# This also extends the textures.\n",
    "meshes = mesh.extend(batch_size)\n",
    "\n",
    "# Get a batch of viewing angles.\n",
    "elev = torch.linspace(0, 180, batch_size)\n",
    "azim = torch.linspace(-180, 180, batch_size)\n",
    "\n",
    "# All the cameras helper methods support mixed type inputs and broadcasting. So we can\n",
    "# view the camera from the same distance and specify dist=2.7 as a float,\n",
    "# and then specify elevation and azimuth angles for each viewpoint as tensors.\n",
    "R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "# Move the light back in front of the cow which is facing the -z direction.\n",
    "lights.location = torch.tensor([[0.0, 0.0, -3.0]], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can pass arbitrary keyword arguments to the rasterizer/shader via the renderer\n",
    "# so the renderer does not need to be reinitialized if any of the settings change.\n",
    "images = renderer(meshes, cameras=cameras, lights=lights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotly visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts, faces_idx, _ = load_obj(obj_filename)\n",
    "faces = faces_idx.verts_idx\n",
    "\n",
    "# Initialize each vertex to be white in color.\n",
    "verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n",
    "textures = TexturesVertex(verts_features=verts_rgb.to(device))\n",
    "\n",
    "# Create a Meshes object\n",
    "mesh = Meshes(verts=[verts.to(device)], faces=[faces.to(device)], textures=textures)\n",
    "\n",
    "# Render the plotly figure\n",
    "fig = plot_scene({\"subplot1\": {\"cow_mesh\": mesh}})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Plotly's default colors (no texture)\n",
    "mesh = Meshes(verts=[verts.to(device)], faces=[faces.to(device)])\n",
    "\n",
    "# Render the plotly figure\n",
    "fig = plot_scene({\"subplot1\": {\"cow_mesh\": mesh}})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch of meshes, and offset one to prevent overlap\n",
    "mesh_batch = Meshes(\n",
    "    verts=[verts.to(device), (verts + 2).to(device)], faces=[faces.to(device), faces.to(device)]\n",
    ")\n",
    "\n",
    "# plot mesh batch in the same trace\n",
    "fig = plot_scene({\"subplot1\": {\"cow_mesh_batch\": mesh_batch}})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render terrain map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from lac.utils.plotting import plot_surface, plot_poses, plot_3d_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = np.load(\"../../../data/heightmaps/competition/Moon_Map_01_preset_0.dat\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structured_grid_to_pytorch3d_mesh(grid):\n",
    "    \"\"\"\n",
    "    Convert an NxNx3 structured grid into a triangular mesh in PyTorch3D.\n",
    "\n",
    "    Args:\n",
    "        grid (np.ndarray): Shape (N, N, 3), regular 2.5D grid of (x, y, z) points.\n",
    "\n",
    "    Returns:\n",
    "        Meshes: A PyTorch3D mesh object.\n",
    "    \"\"\"\n",
    "    N, M, _ = grid.shape  # N x M grid of (x,y,z) points\n",
    "    vertices = grid.reshape(-1, 3)  # Flatten into (N*M, 3)\n",
    "\n",
    "    # Generate face indices for a regular grid of quads split into triangles\n",
    "    faces = []\n",
    "    for i in range(N - 1):\n",
    "        for j in range(M - 1):\n",
    "            # Get indices in the flattened array\n",
    "            v0 = i * M + j\n",
    "            v1 = i * M + (j + 1)\n",
    "            v2 = (i + 1) * M + j\n",
    "            v3 = (i + 1) * M + (j + 1)\n",
    "\n",
    "            # Each quad is split into two triangles\n",
    "            faces.append([v0, v1, v2])  # Lower-left triangle\n",
    "            faces.append([v1, v3, v2])  # Upper-right triangle\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    verts = torch.tensor(vertices, dtype=torch.float32)\n",
    "    faces = torch.tensor(faces, dtype=torch.int64)\n",
    "\n",
    "    # Create PyTorch3D Mesh\n",
    "    mesh = Meshes(verts=[verts], faces=[faces])\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = structured_grid_to_pytorch3d_mesh(map[..., :3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the plotly figure\n",
    "fig = plot_scene({\"subplot1\": {\"map_mesh\": mesh}})\n",
    "fig.update_layout(width=1200, height=700, scene=dict(aspectmode=\"cube\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Fit a mesh with texture via rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch3d.utils import ico_sphere\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, save_obj\n",
    "\n",
    "from pytorch3d.loss import (\n",
    "    chamfer_distance,\n",
    "    mesh_edge_loss,\n",
    "    mesh_laplacian_smoothing,\n",
    "    mesh_normal_consistency,\n",
    ")\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights,\n",
    "    DirectionalLights,\n",
    "    Materials,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader,\n",
    "    SoftSilhouetteShader,\n",
    "    SoftPhongShader,\n",
    "    TexturesVertex,\n",
    ")\n",
    "\n",
    "# add path for demo utils functions\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = \"./data\"\n",
    "obj_filename = os.path.join(DATA_DIR, \"cow_mesh/cow.obj\")\n",
    "\n",
    "# Load obj file\n",
    "mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "\n",
    "# We scale normalize and center the target mesh to fit in a sphere of radius 1\n",
    "# centered at (0,0,0). (scale, center) will be used to bring the predicted mesh\n",
    "# to its original center and scale.  Note that normalizing the target mesh,\n",
    "# speeds up the optimization but is not necessary!\n",
    "verts = mesh.verts_packed()\n",
    "N = verts.shape[0]\n",
    "center = verts.mean(0)\n",
    "scale = max((verts - center).abs().max(0)[0])\n",
    "mesh.offset_verts_(-center)\n",
    "mesh.scale_verts_((1.0 / float(scale)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of different viewpoints from which we want to render the mesh.\n",
    "num_views = 20\n",
    "\n",
    "# Get a batch of viewing angles.\n",
    "elev = torch.linspace(0, 360, num_views)\n",
    "azim = torch.linspace(-180, 180, num_views)\n",
    "\n",
    "# Place a point light in front of the object. As mentioned above, the front of\n",
    "# the cow is facing the -z direction.\n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "# Initialize an OpenGL perspective camera that represents a batch of different\n",
    "# viewing angles. All the cameras helper methods support mixed type inputs and\n",
    "# broadcasting. So we can view the camera from the a distance of dist=2.7, and\n",
    "# then specify elevation and azimuth angles for each viewpoint as tensors.\n",
    "R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "# We arbitrarily choose one particular view that will be used to visualize\n",
    "# results\n",
    "camera = FoVPerspectiveCameras(device=device, R=R[None, 1, ...], T=T[None, 1, ...])\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output\n",
    "# image to be of size 128X128. As we are rendering images for visualization\n",
    "# purposes only we will set faces_per_pixel=1 and blur_radius=0.0. Refer to\n",
    "# rasterize_meshes.py for explanations of these parameters.  We also leave\n",
    "# bin_size and max_faces_per_bin to their default values of None, which sets\n",
    "# their values using heuristics and ensures that the faster coarse-to-fine\n",
    "# rasterization method is used.  Refer to docs/notes/renderer.md for an\n",
    "# explanation of the difference between naive and coarse-to-fine rasterization.\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=128,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    ")\n",
    "\n",
    "# Create a Phong renderer by composing a rasterizer and a shader. The textured\n",
    "# Phong shader will interpolate the texture uv coordinates for each vertex,\n",
    "# sample from a texture image and apply the Phong lighting model\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(cameras=camera, raster_settings=raster_settings),\n",
    "    shader=SoftPhongShader(device=device, cameras=camera, lights=lights),\n",
    ")\n",
    "\n",
    "# Create a batch of meshes by repeating the cow mesh and associated textures.\n",
    "# Meshes has a useful `extend` method which allows us do this very easily.\n",
    "# This also extends the textures.\n",
    "meshes = mesh.extend(num_views)\n",
    "\n",
    "# Render the cow mesh from each viewing angle\n",
    "target_images = renderer(meshes, cameras=cameras, lights=lights)\n",
    "\n",
    "# Our multi-view cow dataset will be represented by these 2 lists of tensors,\n",
    "# each of length num_views.\n",
    "target_rgb = [target_images[i, ..., :3] for i in range(num_views)]\n",
    "target_cameras = [\n",
    "    FoVPerspectiveCameras(device=device, R=R[None, i, ...], T=T[None, i, ...])\n",
    "    for i in range(num_views)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(target_rgb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(target_rgb[0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.utils.visualization import image_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB images\n",
    "image_grid(target_images.cpu().numpy(), rows=4, cols=5, rgb=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterization settings for silhouette rendering\n",
    "sigma = 1e-4\n",
    "raster_settings_silhouette = RasterizationSettings(\n",
    "    image_size=128,\n",
    "    blur_radius=np.log(1.0 / 1e-4 - 1.0) * sigma,\n",
    "    faces_per_pixel=50,\n",
    ")\n",
    "\n",
    "# Silhouette renderer\n",
    "renderer_silhouette = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(cameras=camera, raster_settings=raster_settings_silhouette),\n",
    "    shader=SoftSilhouetteShader(),\n",
    ")\n",
    "\n",
    "# Render silhouette images.  The 3rd channel of the rendering output is\n",
    "# the alpha/silhouette channel\n",
    "silhouette_images = renderer_silhouette(meshes, cameras=cameras, lights=lights)\n",
    "target_silhouette = [silhouette_images[i, ..., 3] for i in range(num_views)]\n",
    "\n",
    "# Visualize silhouette images\n",
    "image_grid(silhouette_images.cpu().numpy(), rows=4, cols=5, rgb=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mesh prediction via silhouette rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a visualization comparing the rendered predicted mesh to the ground truth\n",
    "# mesh\n",
    "def visualize_prediction(\n",
    "    predicted_mesh,\n",
    "    renderer=renderer_silhouette,\n",
    "    target_image=target_rgb[1],\n",
    "    title=\"\",\n",
    "    silhouette=False,\n",
    "):\n",
    "    inds = 3 if silhouette else range(3)\n",
    "    with torch.no_grad():\n",
    "        predicted_images = renderer(predicted_mesh)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(predicted_images[0, ..., inds].cpu().detach().numpy())\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(target_image.cpu().detach().numpy())\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "\n",
    "# Plot losses as a function of optimization iteration\n",
    "def plot_losses(losses):\n",
    "    fig = plt.figure(figsize=(13, 5))\n",
    "    ax = fig.gca()\n",
    "    for k, l in losses.items():\n",
    "        ax.plot(l[\"values\"], label=k + \" loss\")\n",
    "    ax.legend(fontsize=\"16\")\n",
    "    ax.set_xlabel(\"Iteration\", fontsize=\"16\")\n",
    "    ax.set_ylabel(\"Loss\", fontsize=\"16\")\n",
    "    ax.set_title(\"Loss vs iterations\", fontsize=\"16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize the source shape to be a sphere of radius 1.\n",
    "src_mesh = ico_sphere(4, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterization settings for differentiable rendering, where the blur_radius\n",
    "# initialization is based on Liu et al, 'Soft Rasterizer: A Differentiable\n",
    "# Renderer for Image-based 3D Reasoning', ICCV 2019\n",
    "sigma = 1e-4\n",
    "raster_settings_soft = RasterizationSettings(\n",
    "    image_size=128,\n",
    "    blur_radius=np.log(1.0 / 1e-4 - 1.0) * sigma,\n",
    "    faces_per_pixel=50,\n",
    ")\n",
    "\n",
    "# Silhouette renderer\n",
    "renderer_silhouette = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(cameras=camera, raster_settings=raster_settings_soft),\n",
    "    shader=SoftSilhouetteShader(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of views to optimize over in each SGD iteration\n",
    "num_views_per_iteration = 2\n",
    "# Number of optimization steps\n",
    "Niter = 2000\n",
    "# Plot period for the losses\n",
    "plot_period = 250\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Optimize using rendered silhouette image loss, mesh edge loss, mesh normal\n",
    "# consistency, and mesh laplacian smoothing\n",
    "losses = {\n",
    "    \"silhouette\": {\"weight\": 1.0, \"values\": []},\n",
    "    \"edge\": {\"weight\": 1.0, \"values\": []},\n",
    "    \"normal\": {\"weight\": 0.01, \"values\": []},\n",
    "    \"laplacian\": {\"weight\": 1.0, \"values\": []},\n",
    "}\n",
    "\n",
    "\n",
    "# Losses to smooth / regularize the mesh shape\n",
    "def update_mesh_shape_prior_losses(mesh, loss):\n",
    "    # and (b) the edge length of the predicted mesh\n",
    "    loss[\"edge\"] = mesh_edge_loss(mesh)\n",
    "\n",
    "    # mesh normal consistency\n",
    "    loss[\"normal\"] = mesh_normal_consistency(mesh)\n",
    "\n",
    "    # mesh laplacian smoothing\n",
    "    loss[\"laplacian\"] = mesh_laplacian_smoothing(mesh, method=\"uniform\")\n",
    "\n",
    "\n",
    "# We will learn to deform the source mesh by offsetting its vertices\n",
    "# The shape of the deform parameters is equal to the total number of vertices in\n",
    "# src_mesh\n",
    "verts_shape = src_mesh.verts_packed().shape\n",
    "deform_verts = torch.full(verts_shape, 0.0, device=device, requires_grad=True)\n",
    "\n",
    "# The optimizer\n",
    "optimizer = torch.optim.SGD([deform_verts], lr=1.0, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = tqdm(range(Niter))\n",
    "\n",
    "for i in loop:\n",
    "    # Initialize optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Deform the mesh\n",
    "    new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "\n",
    "    # Losses to smooth /regularize the mesh shape\n",
    "    loss = {k: torch.tensor(0.0, device=device) for k in losses}\n",
    "    update_mesh_shape_prior_losses(new_src_mesh, loss)\n",
    "\n",
    "    # Compute the average silhouette loss over two random views, as the average\n",
    "    # squared L2 distance between the predicted silhouette and the target\n",
    "    # silhouette from our dataset\n",
    "    for j in np.random.permutation(num_views).tolist()[:num_views_per_iteration]:\n",
    "        images_predicted = renderer_silhouette(\n",
    "            new_src_mesh, cameras=target_cameras[j], lights=lights\n",
    "        )\n",
    "        predicted_silhouette = images_predicted[..., 3]\n",
    "        loss_silhouette = ((predicted_silhouette - target_silhouette[j]) ** 2).mean()\n",
    "        loss[\"silhouette\"] += loss_silhouette / num_views_per_iteration\n",
    "\n",
    "    # Weighted sum of the losses\n",
    "    sum_loss = torch.tensor(0.0, device=device)\n",
    "    for k, l in loss.items():\n",
    "        sum_loss += l * losses[k][\"weight\"]\n",
    "        losses[k][\"values\"].append(float(l.detach().cpu()))\n",
    "\n",
    "    # Print the losses\n",
    "    loop.set_description(\"total_loss = %.6f\" % sum_loss)\n",
    "\n",
    "    # Plot mesh\n",
    "    if i % plot_period == 0:\n",
    "        visualize_prediction(\n",
    "            new_src_mesh, title=\"iter: %d\" % i, silhouette=True, target_image=target_silhouette[1]\n",
    "        )\n",
    "\n",
    "    # Optimization step\n",
    "    sum_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(new_src_mesh, silhouette=True, target_image=target_silhouette[1])\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mesh and texture prediction via textured mesh rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterization settings for differentiable rendering, where the blur_radius\n",
    "# initialization is based on Liu et al, 'Soft Rasterizer: A Differentiable\n",
    "# Renderer for Image-based 3D Reasoning', ICCV 2019\n",
    "sigma = 1e-4\n",
    "raster_settings_soft = RasterizationSettings(\n",
    "    image_size=128,\n",
    "    blur_radius=np.log(1.0 / 1e-4 - 1.0) * sigma,\n",
    "    faces_per_pixel=50,\n",
    "    perspective_correct=False,\n",
    ")\n",
    "\n",
    "# Differentiable soft renderer using per vertex RGB colors for texture\n",
    "renderer_textured = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(cameras=camera, raster_settings=raster_settings_soft),\n",
    "    shader=SoftPhongShader(device=device, cameras=camera, lights=lights),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of views to optimize over in each SGD iteration\n",
    "num_views_per_iteration = 2\n",
    "# Number of optimization steps\n",
    "Niter = 2000\n",
    "# Plot period for the losses\n",
    "plot_period = 250\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Optimize using rendered RGB image loss, rendered silhouette image loss, mesh\n",
    "# edge loss, mesh normal consistency, and mesh laplacian smoothing\n",
    "losses = {\n",
    "    \"rgb\": {\"weight\": 1.0, \"values\": []},\n",
    "    \"silhouette\": {\"weight\": 1.0, \"values\": []},\n",
    "    \"edge\": {\"weight\": 1.0, \"values\": []},\n",
    "    \"normal\": {\"weight\": 0.01, \"values\": []},\n",
    "    \"laplacian\": {\"weight\": 1.0, \"values\": []},\n",
    "}\n",
    "\n",
    "# We will learn to deform the source mesh by offsetting its vertices\n",
    "# The shape of the deform parameters is equal to the total number of vertices in\n",
    "# src_mesh\n",
    "verts_shape = src_mesh.verts_packed().shape\n",
    "deform_verts = torch.full(verts_shape, 0.0, device=device, requires_grad=True)\n",
    "\n",
    "# We will also learn per vertex colors for our sphere mesh that define texture\n",
    "# of the mesh\n",
    "sphere_verts_rgb = torch.full([1, verts_shape[0], 3], 0.5, device=device, requires_grad=True)\n",
    "\n",
    "# The optimizer\n",
    "optimizer = torch.optim.SGD([deform_verts, sphere_verts_rgb], lr=1.0, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = tqdm(range(Niter))\n",
    "\n",
    "for i in loop:\n",
    "    # Initialize optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Deform the mesh\n",
    "    new_src_mesh = src_mesh.offset_verts(deform_verts)\n",
    "\n",
    "    # Add per vertex colors to texture the mesh\n",
    "    new_src_mesh.textures = TexturesVertex(verts_features=sphere_verts_rgb)\n",
    "\n",
    "    # Losses to smooth /regularize the mesh shape\n",
    "    loss = {k: torch.tensor(0.0, device=device) for k in losses}\n",
    "    update_mesh_shape_prior_losses(new_src_mesh, loss)\n",
    "\n",
    "    # Randomly select two views to optimize over in this iteration.  Compared\n",
    "    # to using just one view, this helps resolve ambiguities between updating\n",
    "    # mesh shape vs. updating mesh texture\n",
    "    for j in np.random.permutation(num_views).tolist()[:num_views_per_iteration]:\n",
    "        images_predicted = renderer_textured(new_src_mesh, cameras=target_cameras[j], lights=lights)\n",
    "\n",
    "        # Squared L2 distance between the predicted silhouette and the target\n",
    "        # silhouette from our dataset\n",
    "        predicted_silhouette = images_predicted[..., 3]\n",
    "        loss_silhouette = ((predicted_silhouette - target_silhouette[j]) ** 2).mean()\n",
    "        loss[\"silhouette\"] += loss_silhouette / num_views_per_iteration\n",
    "\n",
    "        # Squared L2 distance between the predicted RGB image and the target\n",
    "        # image from our dataset\n",
    "        predicted_rgb = images_predicted[..., :3]\n",
    "        loss_rgb = ((predicted_rgb - target_rgb[j]) ** 2).mean()\n",
    "        loss[\"rgb\"] += loss_rgb / num_views_per_iteration\n",
    "\n",
    "    # Weighted sum of the losses\n",
    "    sum_loss = torch.tensor(0.0, device=device)\n",
    "    for k, l in loss.items():\n",
    "        sum_loss += l * losses[k][\"weight\"]\n",
    "        losses[k][\"values\"].append(float(l.detach().cpu()))\n",
    "\n",
    "    # Print the losses\n",
    "    loop.set_description(\"total_loss = %.6f\" % sum_loss)\n",
    "\n",
    "    # Plot mesh\n",
    "    if i % plot_period == 0:\n",
    "        visualize_prediction(\n",
    "            new_src_mesh, renderer=renderer_textured, title=\"iter: %d\" % i, silhouette=False\n",
    "        )\n",
    "\n",
    "    # Optimization step\n",
    "    sum_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(new_src_mesh, renderer=renderer_textured, silhouette=False)\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the verts and faces of the final predicted mesh\n",
    "final_verts, final_faces = new_src_mesh.get_mesh_verts_faces(0)\n",
    "\n",
    "# Scale normalize back to the original target size\n",
    "final_verts = final_verts * scale + center\n",
    "\n",
    "# Store the predicted mesh using save_obj\n",
    "final_obj = os.path.join(\"./\", \"final_model.obj\")\n",
    "save_obj(final_obj, final_verts, final_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit terrain mesh from rover images\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
