{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit terrain mesh from rover images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch3d.vis.plotly_vis import plot_scene\n",
    "\n",
    "from pytorch3d.renderer import (\n",
    "    PointLights,\n",
    "    BlendParams,\n",
    "    DirectionalLights,\n",
    "    FoVPerspectiveCameras,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader,\n",
    "    TexturesVertex,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "from pytorch3d_utils import structured_grid_to_pytorch3d_mesh\n",
    "from lac.utils.plotting import plot_poses, plot_surface, plot_mesh\n",
    "from lac.utils.frames import get_cam_pose_rover, CAMERA_TO_OPENCV_PASSIVE, invert_transform_mat\n",
    "from lac.util import load_data\n",
    "from lac.params import IMG_FOV_RAD\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = np.load(\"../../../data/heightmaps/competition/Moon_Map_01_preset_0.dat\", allow_pickle=True)\n",
    "\n",
    "mesh = structured_grid_to_pytorch3d_mesh(map[..., :3])\n",
    "mesh = mesh.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_mesh(mesh)\n",
    "fig.update_layout(width=1200, height=700, scene=dict(aspectmode=\"data\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"../../../output/LocalizationAgent/map1_preset0_4m_spiral\"\n",
    "data_path = \"../../../output/DataCollectionAgent/map1_preset0_nolight_allcams\"\n",
    "initial_pose, lander_pose, poses, imu_data, cam_config = load_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rover_pose_to_cam_pose(rover_pose, cam_name=\"FrontLeft\"):\n",
    "    camera_pose = get_cam_pose_rover(cam_name)\n",
    "    camera_pose[:3, :3] = CAMERA_TO_OPENCV_PASSIVE\n",
    "    return rover_pose @ camera_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't ask me why this works\n",
    "def cam_pose_to_p3d_cam(cam_pose):\n",
    "    R_p3d = cam_pose[:3, :3].T.copy()\n",
    "    # R_p3d[:, 0] *= -1\n",
    "    # R_p3d[:, 1] *= -1\n",
    "    T_p3d = -R_p3d @ cam_pose[:3, 3]\n",
    "    return R_p3d.T, T_p3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "r = Rotation.from_euler(\"XYZ\", (0, -90, 0), degrees=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = []\n",
    "T = []\n",
    "target_rgb = []\n",
    "cam_poses = []\n",
    "\n",
    "start_idx = 100\n",
    "end_idx = 4000\n",
    "increment = 10\n",
    "idxs = np.arange(start_idx, end_idx, increment)\n",
    "num_views = len(idxs)\n",
    "print(\"num_views: \", num_views)\n",
    "\n",
    "for i in idxs:\n",
    "    cam_pose = rover_pose_to_cam_pose(poses[i])\n",
    "    # Convert to pytorch3d convention\n",
    "    cam_pose[:, 0] *= -1\n",
    "    cam_pose[:, 1] *= -1\n",
    "    cam_poses.append(cam_pose)\n",
    "\n",
    "    R_p3d, T_p3d = cam_pose_to_p3d_cam(cam_pose)\n",
    "\n",
    "    R.append(torch.tensor(R_p3d, device=device).float())\n",
    "    T.append(torch.tensor(T_p3d, device=device).float())\n",
    "    img_np = cv2.imread(os.path.join(data_path, \"FrontLeft\", f\"{i}.png\")) / 255.0\n",
    "    new_size = (img_np.shape[1] // 2, img_np.shape[0] // 2)\n",
    "    downscaled_img_np = cv2.resize(img_np, new_size, interpolation=cv2.INTER_AREA)\n",
    "    target_rgb.append(torch.tensor(downscaled_img_np, device=device).float())\n",
    "\n",
    "R = torch.stack(R)\n",
    "T = torch.stack(T)\n",
    "target_cameras = [\n",
    "    FoVPerspectiveCameras(\n",
    "        device=device, R=R[None, i, ...], T=T[None, i, ...], fov=IMG_FOV_RAD, degrees=False\n",
    "    )\n",
    "    for i in range(num_views)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_mesh(mesh)\n",
    "i = 100\n",
    "fig = plot_poses(cam_poses[i : i + 1], fig=fig)\n",
    "fig.update_layout(width=1200, height=700, scene=dict(aspectmode=\"data\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(target_rgb[i].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T, fov=IMG_FOV_RAD, degrees=False)\n",
    "camera = FoVPerspectiveCameras(\n",
    "    device=device, R=R[None, i, ...], T=T[None, i, ...], fov=IMG_FOV_RAD, degrees=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place a point light in front of the object. As mentioned above, the front of\n",
    "# the cow is facing the -z direction.\n",
    "# lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "# lights = PointLights(\n",
    "#     device=device,\n",
    "#     diffuse_color=[[1.0, 1.0, 1.0]],\n",
    "#     ambient_color=[[0.5, 0.5, 0.5]],\n",
    "#     specular_color=[[0.0, 0.0, 0.0]],\n",
    "#     location=[[0.0, 0.0, -3.0]],\n",
    "# )\n",
    "lights = DirectionalLights(\n",
    "    device=device,\n",
    "    diffuse_color=[[2.0, 2.0, 2.0]],\n",
    "    ambient_color=[[0.0, 0.0, 0.0]],\n",
    "    specular_color=[[0.0, 0.0, 0.0]],\n",
    "    direction=[[0.0, 1.0, -0.05]],\n",
    ")\n",
    "\n",
    "# Rasterization settings for differentiable rendering, where the blur_radius\n",
    "# initialization is based on Liu et al, 'Soft Rasterizer: A Differentiable\n",
    "# Renderer for Image-based 3D Reasoning', ICCV 2019\n",
    "sigma = 1e-4\n",
    "raster_settings_soft = RasterizationSettings(\n",
    "    image_size=(360, 640),\n",
    "    blur_radius=np.log(1.0 / 1e-4 - 1.0) * sigma,\n",
    "    faces_per_pixel=50,\n",
    "    perspective_correct=False,\n",
    ")\n",
    "\n",
    "# Differentiable soft renderer using per vertex RGB colors for texture\n",
    "blend_params = BlendParams(background_color=(0.0, 0.0, 0.0))\n",
    "renderer_textured = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(cameras=camera, raster_settings=raster_settings_soft),\n",
    "    shader=SoftPhongShader(device=device, cameras=camera, lights=lights, blend_params=blend_params),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_shape = mesh.verts_packed().shape\n",
    "terrain_verts_rgb = torch.full([1, verts_shape[0], 3], 1.0, device=device)\n",
    "textured_mesh = mesh.clone()\n",
    "textured_mesh.textures = TexturesVertex(terrain_verts_rgb)\n",
    "\n",
    "rendered_image = renderer_textured(textured_mesh, cameras=camera, lights=lights)\n",
    "rendered_rgb = rendered_image[0, ..., :3].cpu().numpy()\n",
    "plt.imshow(rendered_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.loss import (\n",
    "    mesh_edge_loss,\n",
    "    mesh_laplacian_smoothing,\n",
    "    mesh_normal_consistency,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of views to optimize over in each SGD iteration\n",
    "num_views_per_iteration = 2\n",
    "# Number of optimization steps\n",
    "Niter = 2000\n",
    "# Plot period for the losses\n",
    "plot_period = 250\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Optimize using rendered RGB image loss, rendered silhouette image loss, mesh\n",
    "# edge loss, mesh normal consistency, and mesh laplacian smoothing\n",
    "losses = {\n",
    "    \"rgb\": {\"weight\": 1.0, \"values\": []},\n",
    "    \"silhouette\": {\"weight\": 1.0, \"values\": []},\n",
    "    \"edge\": {\"weight\": 1.0, \"values\": []},\n",
    "    \"normal\": {\"weight\": 0.01, \"values\": []},\n",
    "    \"laplacian\": {\"weight\": 1.0, \"values\": []},\n",
    "}\n",
    "\n",
    "\n",
    "# Losses to smooth / regularize the mesh shape\n",
    "def update_mesh_shape_prior_losses(mesh, loss):\n",
    "    # and (b) the edge length of the predicted mesh\n",
    "    loss[\"edge\"] = mesh_edge_loss(mesh)\n",
    "    # mesh normal consistency\n",
    "    loss[\"normal\"] = mesh_normal_consistency(mesh)\n",
    "    # mesh laplacian smoothing\n",
    "    loss[\"laplacian\"] = mesh_laplacian_smoothing(mesh, method=\"uniform\")\n",
    "\n",
    "\n",
    "# Deform the mesh\n",
    "verts_shape = mesh.verts_packed().shape\n",
    "# deform_verts = torch.full(verts_shape, 0.0, device=device, requires_grad=True)\n",
    "deform_zs = torch.zeros(verts_shape[0], device=device, requires_grad=True)\n",
    "\n",
    "# Learn per vertex colors that define texture of the mesh\n",
    "verts_shape = mesh.verts_packed().shape\n",
    "terrain_verts_rgb = torch.full([1, verts_shape[0], 3], 1.0, device=device, requires_grad=True)\n",
    "\n",
    "# The optimizer\n",
    "optimizer = torch.optim.Adam([deform_zs, terrain_verts_rgb], lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a visualization comparing the rendered predicted mesh to the ground truth\n",
    "# mesh\n",
    "def visualize_prediction(\n",
    "    predicted_mesh,\n",
    "    renderer=renderer_textured,\n",
    "    target_image=target_rgb[1],\n",
    "    title=\"\",\n",
    "    silhouette=False,\n",
    "):\n",
    "    inds = 3 if silhouette else range(3)\n",
    "    with torch.no_grad():\n",
    "        predicted_images = renderer(predicted_mesh)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(predicted_images[0, ..., inds].cpu().detach().numpy())\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(target_image.cpu().detach().numpy())\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = tqdm(range(Niter))\n",
    "\n",
    "for i in loop:\n",
    "    # Initialize optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # # Deform the mesh\n",
    "    deform_verts = torch.zeros(verts_shape, device=device)\n",
    "    deform_verts[:, 2] = deform_zs\n",
    "    new_mesh = mesh.offset_verts(deform_verts)\n",
    "    # new_mesh = mesh.clone()\n",
    "\n",
    "    # Add per vertex colors to texture the mesh\n",
    "    new_mesh.textures = TexturesVertex(verts_features=terrain_verts_rgb)\n",
    "\n",
    "    # Losses to smooth /regularize the mesh shape\n",
    "    loss = {k: torch.tensor(0.0, device=device) for k in losses}\n",
    "    update_mesh_shape_prior_losses(new_mesh, loss)\n",
    "\n",
    "    # Randomly select two views to optimize over in this iteration.  Compared\n",
    "    # to using just one view, this helps resolve ambiguities between updating\n",
    "    # mesh shape vs. updating mesh texture\n",
    "    for j in np.random.permutation(num_views).tolist()[:num_views_per_iteration]:\n",
    "        images_predicted = renderer_textured(new_mesh, cameras=target_cameras[j], lights=lights)\n",
    "\n",
    "        # Squared L2 distance between the predicted RGB image and the target\n",
    "        # image from our dataset\n",
    "        predicted_rgb = images_predicted[..., :3]\n",
    "        loss_rgb = ((predicted_rgb - target_rgb[j]) ** 2).mean()\n",
    "        loss[\"rgb\"] += loss_rgb / num_views_per_iteration\n",
    "\n",
    "    # Weighted sum of the losses\n",
    "    sum_loss = torch.tensor(0.0, device=device)\n",
    "    for k, l in loss.items():\n",
    "        sum_loss += l * losses[k][\"weight\"]\n",
    "        losses[k][\"values\"].append(float(l.detach().cpu()))\n",
    "\n",
    "    # Print the losses\n",
    "    loop.set_description(\"total_loss = %.6f\" % sum_loss)\n",
    "\n",
    "    # Plot mesh\n",
    "    if i % plot_period == 0:\n",
    "        visualize_prediction(\n",
    "            new_mesh, renderer=renderer_textured, title=\"iter: %d\" % i, silhouette=False\n",
    "        )\n",
    "\n",
    "    # Optimization step\n",
    "    sum_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mesh = mesh.offset_verts(deform_verts)\n",
    "final_mesh.textures = TexturesVertex(verts_features=terrain_verts_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mesh.verts_packed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_mesh(final_mesh, show_edges=False, textured=True)\n",
    "fig.update_layout(width=1200, height=700, scene=dict(aspectmode=\"data\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses as a function of optimization iteration\n",
    "def plot_losses(losses):\n",
    "    fig = plt.figure(figsize=(13, 5))\n",
    "    ax = fig.gca()\n",
    "    for k, l in losses.items():\n",
    "        ax.plot(l[\"values\"], label=k + \" loss\")\n",
    "    ax.legend(fontsize=\"16\")\n",
    "    ax.set_xlabel(\"Iteration\", fontsize=\"16\")\n",
    "    ax.set_ylabel(\"Loss\", fontsize=\"16\")\n",
    "    ax.set_title(\"Loss vs iterations\", fontsize=\"16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(new_mesh, renderer=renderer_textured, silhouette=False)\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
