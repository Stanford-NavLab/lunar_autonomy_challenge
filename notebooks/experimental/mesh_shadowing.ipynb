{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch3d.io import load_objs_as_meshes\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras,\n",
    "    PointLights,\n",
    "    DirectionalLights,\n",
    "    RasterizationSettings,\n",
    "    MeshRenderer,\n",
    "    MeshRasterizer,\n",
    "    SoftPhongShader,\n",
    "    HardPhongShader,\n",
    ")\n",
    "\n",
    "# add path for demo utils functions\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set paths\n",
    "DATA_DIR = \"./data\"\n",
    "obj_filename = os.path.join(DATA_DIR, \"cow_mesh/cow.obj\")\n",
    "\n",
    "# Load obj file\n",
    "mesh = load_objs_as_meshes([obj_filename], device=device)\n",
    "\n",
    "# We scale normalize and center the target mesh to fit in a sphere of radius 1\n",
    "# centered at (0,0,0). (scale, center) will be used to bring the predicted mesh\n",
    "# to its original center and scale.  Note that normalizing the target mesh,\n",
    "# speeds up the optimization but is not necessary!\n",
    "verts = mesh.verts_packed()\n",
    "N = verts.shape[0]\n",
    "center = verts.mean(0)\n",
    "scale = max((verts - center).abs().max(0)[0])\n",
    "mesh.offset_verts_(-center)\n",
    "mesh.scale_verts_((1.0 / float(scale)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lac.utils.plotting import plot_poses, plot_mesh\n",
    "\n",
    "fig = plot_mesh(mesh)\n",
    "fig.update_layout(width=1200, height=700, scene=dict(aspectmode=\"data\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of different viewpoints from which we want to render the mesh.\n",
    "num_views = 20\n",
    "\n",
    "elev = torch.linspace(0, 360, num_views)\n",
    "azim = torch.linspace(-180, 180, num_views)\n",
    "\n",
    "# lights = PointLights(\n",
    "#     device=device,\n",
    "#     diffuse_color=[[1.0, 1.0, 1.0]],\n",
    "#     ambient_color=[[0.0, 0.0, 0.0]],\n",
    "#     specular_color=[[0.0, 0.0, 0.0]],\n",
    "#     location=[[0.0, 0.0, -3.0]],\n",
    "# )\n",
    "lights = DirectionalLights(\n",
    "    device=device,\n",
    "    diffuse_color=[[1.0, 1.0, 1.0]],\n",
    "    ambient_color=[[0.0, 0.0, 0.0]],\n",
    "    specular_color=[[0.0, 0.0, 0.0]],\n",
    "    direction=[[0.0, 0.0, -1.0]],\n",
    ")\n",
    "\n",
    "R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
    "cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
    "camera = FoVPerspectiveCameras(device=device, R=R[None, 1, ...], T=T[None, 1, ...])\n",
    "\n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=128,\n",
    "    blur_radius=0.0,\n",
    "    faces_per_pixel=1,\n",
    ")\n",
    "\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(cameras=camera, raster_settings=raster_settings),\n",
    "    shader=HardPhongShader(device=device, cameras=camera, lights=lights),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image = renderer(mesh, cameras=camera, lights=lights)\n",
    "target_rgb = target_image[0, ..., :3].cpu().numpy()\n",
    "plt.imshow(target_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lights.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Step 1: Render Depth Map from Light's Perspective\n",
    "# ----------------------------------------\n",
    "light_camera = FoVPerspectiveCameras(\n",
    "    device=device, R=torch.eye(3).unsqueeze(0).to(device), T=-lights.location, znear=0.1\n",
    ")\n",
    "\n",
    "light_raster_settings = RasterizationSettings(image_size=128, blur_radius=0.0, faces_per_pixel=1)\n",
    "\n",
    "light_rasterizer = MeshRasterizer(cameras=light_camera, raster_settings=light_raster_settings)\n",
    "\n",
    "# Render depth from light's view\n",
    "fragments = light_rasterizer(mesh)\n",
    "depth_map = fragments.zbuf.squeeze(0).min(dim=-1)[0]  # Get min depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(depth_map.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_shadows(mesh, cameras, light_camera, depth_map, image_size=128):\n",
    "    # Step 1: Transform vertices into light's view to check occlusion\n",
    "    verts_world = mesh.verts_packed()  # (V, 3)\n",
    "    verts_light = light_camera.get_world_to_view_transform().transform_points(verts_world)  # (V, 3)\n",
    "\n",
    "    # Step 2: Project vertices into light's image space\n",
    "    verts_light_ndc = light_camera.transform_points(verts_world)  # (V, 3)\n",
    "    pix_coords = (verts_light_ndc[:, :2] + 1) * 0.5 * (image_size - 1)  # Scale to [0, image_size]\n",
    "\n",
    "    # Step 3: Clamp pixel coordinates to avoid out-of-bounds access\n",
    "    pix_coords = pix_coords.clamp(0, image_size - 1)\n",
    "\n",
    "    pix_x = pix_coords[:, 0].long()\n",
    "    pix_y = pix_coords[:, 1].long()\n",
    "\n",
    "    # Ensure depth map is on the same device as pix_x and pix_y\n",
    "    depth_map = depth_map.to(verts_light.device)\n",
    "\n",
    "    # Step 4: Sample depth map at projected pixel locations\n",
    "    sampled_depth = depth_map[pix_y, pix_x]\n",
    "\n",
    "    # Step 5: Compare depths to determine shadows\n",
    "    is_shadowed = verts_light[:, 2] > sampled_depth  # Shadow if light depth < surface depth\n",
    "\n",
    "    # Step 6: Render mesh and apply shadows\n",
    "    fragments = renderer.rasterizer(mesh)\n",
    "    images = renderer.shader(fragments, mesh, lights=lights, cameras=cameras)\n",
    "\n",
    "    # Apply shadow mask (darken shadowed areas)\n",
    "    shadow_mask = is_shadowed[fragments.pix_to_face].squeeze(-1)\n",
    "    shadow_factor = torch.where(shadow_mask, 0.2, 1.0).unsqueeze(-1)  # Darken by 80% in shadow\n",
    "    shadowed_image = images[..., :3] * shadow_factor\n",
    "\n",
    "    return shadowed_image\n",
    "\n",
    "\n",
    "# Render with shadows applied\n",
    "shadowed_image = apply_shadows(mesh, camera, light_camera, depth_map)\n",
    "plt.imshow(shadowed_image[0, ..., :3].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lac-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
